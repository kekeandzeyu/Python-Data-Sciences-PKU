{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第十次课后练习 之二（选做）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**负责助教：朱轩宇**\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">请将作业文件命名为 第十次课后练习-选做题+姓名+学号.ipynb, 例如 第十次课后练习-选做题+张三+1000000000.ipynb</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 社交媒体评论主题分析与可视化\n",
    "\n",
    "## 引言\n",
    "在信息化社会背景下，社交媒体平台成为了人们表达情感、分享观点的重要渠道。通过对社交媒体评论进行情感分析，可以深入了解用户的态度和情感趋势，为市场策略、用户体验优化等提供有力支持。本文将详细介绍如何使用Python进行社交媒体评论的情感分析，并结合数据可视化技术，将分析结果直观地呈现出来。\n",
    "\n",
    "## 实验步骤\n",
    "1. 数据清洗\n",
    "   - 使用pandas读取数据文件，并进行数据清洗和预处理，包括去除重复值、正则清洗和分词。\n",
    "2. 主要关注点分析\n",
    "   - 计算词频并生成词云图，统计文本中词语的出现频率，并使用WordCloud库生成词云图展示结果。\n",
    "3. 主题分析\n",
    "   1. 进行一致性和困惑度计算，通过改变主题数量范围，计算不同主题数量下的一致性和困惑度，并绘制折线图展示结果。\n",
    "   2. 使用TF-IDF模型提取文本的关键词，计算每个关键词在文本中的权重，并输出前30个关键词。\n",
    "   3. 进行主题建模和关键词提取，使用LDA模型对分词结果进行主题建模，并提取每个主题的关键词。\n",
    "   4. 对主题建模结果进行可视化，使用pyLDAvis库生成LDA主题模型的可视化结果，并保存为HTML文件。\n",
    "   5. 聚类分析\n",
    "\n",
    "实验提供了两份数据，一个比较大，一个比较小，可以先使用小数据跑通之后换更大的数据运行。\n",
    "\n",
    "### 实验使用的关键库及版本\n",
    "python 3.10\n",
    "- jieba                     0.42.1\n",
    "- numpy                     1.26.4\n",
    "- pandas                    2.2.3\n",
    "- pyLDAvis                  3.4.1\n",
    "- scikit-learn              1.6.1\n",
    "- seaborn                   0.13.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as psg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pandas读取数据文件，并进行数据清洗和预处理，包括去除重复值、正则清洗和分词。\n",
    "print(\"Loading data...\")\n",
    "data=pd.read_csv(\"./data/doc_ewujushi_short.csv\")\n",
    "stopword_list = []\n",
    "with open(\"./stop_dic/stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        stopword_list.append(line.strip())  # 去除换行符和空格\n",
    "stopwords = set(stopword_list)\n",
    "\n",
    "# 定义分词函数\n",
    "def chinese_word_cut(mytext):\n",
    "    ##############################\n",
    "    # 定义分词函数，实现去除非中文字符和停用词\n",
    "    mytext = re.sub(r'[^\\u4e00-\\u9fa5]', '', mytext)   # 去除非中文字符\n",
    "    words = psg.cut(mytext)  # 使用jieba进行分词\n",
    "    ##############################\n",
    "    return \" \".join([word for word, flag in words if word not in stopwords])  # 去除停用词\n",
    "\n",
    "# 对数据进行处理\n",
    "print(\"Processing data...\")\n",
    "##############################\n",
    "# 对数据进行处理，包括去除重复值、缺失值以及重置索引\n",
    "  # 去除重复值\n",
    "data = data.drop_duplicates()\n",
    "  # 去除缺失值\n",
    "data = data.dropna()\n",
    "  # 重置索引\n",
    "data = data.reset_index(drop=True)\n",
    "##################################\n",
    "\n",
    "print(\"Cleaning data...\")\n",
    "data[\"content_cutted\"] = data.content.apply(chinese_word_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 探索性数据分析(词频分析)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for index, row in data.iterrows():\n",
    "    words = row[\"content_cutted\"].split()\n",
    "    for word in words:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "\n",
    "# 排序\n",
    "word_count_sorted = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "# 筛选高频词\n",
    "high_frequency_num = 10  # 设定筛选的高频词数量\n",
    "high_frequency_words = []\n",
    "##############################\n",
    "# 筛选高频词\n",
    "high_frequency_words = [word for word, count in word_count_sorted[:high_frequency_num]]\n",
    "##############################\n",
    "high_frequency_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化展示\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 中文 字体设置\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False \n",
    "plt.rcParams.update({'font.size': 20}) # 设置字体大小\n",
    "\n",
    "##############################\n",
    "# 生成词频统计图，取前10个高频词\n",
    "high_frequency_words_to_draw = word_count_sorted[:10] # 取前10个高频词\n",
    "words = [word for word, freq in high_frequency_words_to_draw]\n",
    "freqs = [freq for word, freq in high_frequency_words_to_draw]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(words, freqs)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Statistics\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# 生成词云图\n",
    "wordcloud = WordCloud(font_path='simhei.ttf', background_color='white').generate_from_frequencies(word_count)\n",
    "##############################\n",
    "# 显示词云图\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LDA主题分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA主题模型分析\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##############################\n",
    "# 使用CountVectorizer进行文本向量化\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data[\"content_cutted\"])\n",
    "\n",
    "# 设定LDA模型参数，并进行拟合\n",
    "n_components = 5  # 主题数量\n",
    "lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
    "lda.fit(X)\n",
    "##############################\n",
    "\n",
    "# 输出主题词\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    tword = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        topic_w = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        tword.append(topic_w)\n",
    "        print(topic_w)\n",
    "    return tword\n",
    "\n",
    "n_top_words = 25\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "topic_word = print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "topics=lda.transform(X)\n",
    "topic = []\n",
    "##############################\n",
    "# 得到每篇文章对应主题 \n",
    "# 取主题概率最大的主题作为文章的主题\n",
    "topic = [np.argmax(prob) for prob in topics]\n",
    "##############################\n",
    "data['topic']=topic\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[0]#可以看出属于第几个话题的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "pyLDAvis.enable_notebook()                                  #在notebook中展示   \n",
    "pic = pyLDAvis.lda_model.prepare(lda, X, vectorizer)\n",
    "pyLDAvis.save_html(pic, 'lda_pass'+str(n_components)+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 困惑度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plexs = []\n",
    "scores = []\n",
    "n_max_topics = 10\n",
    "#################################\n",
    "# 计算困惑度和对数似然函数\n",
    "for i in range(1,n_max_topics):\n",
    "    print('正在进行第',i,'轮计算')\n",
    "    lda = LatentDirichletAllocation(n_components=i, random_state=42)\n",
    "    lda.fit(X)\n",
    "    plexs.append(lda.perplexity(X))\n",
    "    scores.append(lda.score(X))\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t=9                                      #区间最右侧的值。注意：不能大于n_max_topics\n",
    "x=list(range(1,n_t))\n",
    "plt.plot(x,plexs[1:n_t])\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t=9                                         #区间最右侧的值。注意：不能大于n_max_topics\n",
    "x=list(range(1,n_t))\n",
    "plt.plot(x,scores[1:n_t])\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 聚类分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类分析\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 使用TF-IDF方法表示文本特征\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(data[\"content_cutted\"])\n",
    "\n",
    "# 设定KMeans聚类参数\n",
    "n_clusters = 3  # 聚类数量\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 获取每个聚类的关键特征\n",
    "def get_top_keywords(model, feature_names, n_top_words):\n",
    "    \n",
    "    top_keywords = []\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    ##############################\n",
    "    # 实现函数，获取每个聚类的关键特征\n",
    "    # 通过聚类中心的特征值来获取每个聚类的关键词\n",
    "    for i in range(model.n_clusters):\n",
    "        keywords = [feature_names[ind] for ind in order_centroids[i, :n_top_words]]\n",
    "        top_keywords.append(keywords)\n",
    "    ##############################\n",
    "    return top_keywords\n",
    "\n",
    "top_keywords = get_top_keywords(kmeans, vectorizer.get_feature_names_out(), 10)\n",
    "for i, keywords in enumerate(top_keywords):\n",
    "    print(f\"Cluster {i}: {', '.join(keywords)}\")\n",
    "\n",
    "# 获取每个样本所属的聚类\n",
    "data[\"cluster\"] = kmeans.labels_\n",
    "data[\"cluster\"].value_counts()  # 查看每个聚类的样本数量\n",
    "\n",
    "# 降维和可视化\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "##############################\n",
    "# 使用PCA降维到2维\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "##############################\n",
    "\n",
    "# 创建DataFrame用于可视化\n",
    "df = pd.DataFrame(X_pca, columns=[\"x\", \"y\"])\n",
    "df[\"cluster\"] = data[\"cluster\"]\n",
    "df[\"topic\"] = data[\"topic\"]\n",
    "# 可视化聚类结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"cluster\", palette=\"Set1\", alpha=0.7)\n",
    "plt.title(\"KMeans Clustering Results\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请合理使用AI工具对上述媒体内容进行综述，给出你的步骤（调用方案）以及提示词和结果。\n",
    "\n",
    "结果示例：\n",
    "\n",
    "国际局势动荡下的全球经济与市场反应\n",
    "——乌克兰危机升级，避险情绪推高黄金原油价格\n",
    "\n",
    "近期，乌克兰局势持续紧张，俄罗斯与西方国家的对峙进一步升级，引发全球市场剧烈震荡。据央视新闻报道，乌克兰东部地区冲突加剧，美国总统拜登与北约盟国宣布对俄罗斯实施新一轮经济制裁，导致国际油价和黄金价格大幅上涨。\n",
    "\n",
    "市场影响\n",
    "\n",
    "能源市场：受紧张局势影响，布伦特原油价格突破每桶100美元，欧洲天然气价格飙升。分析人士指出，俄罗斯作为全球主要能源出口国，若冲突持续，可能进一步扰乱供应链。\n",
    "\n",
    "避险资产：黄金价格创下近8个月新高，美元指数走强。金灿荣教授在接受采访时表示，市场避险情绪浓厚，投资者纷纷转向黄金等安全资产。\n",
    "\n",
    "股市震荡：美股三大指数集体下跌，纳指跌幅超2%。A股市场同样受到波及，沪指失守3500点。\n",
    "\n",
    "国际反应\n",
    "俄罗斯总统普京在公开讲话中指责北约东扩威胁国家安全，而乌克兰政府则呼吁国际社会提供更多支持。微博上，#乌克兰局势#话题阅读量突破10亿，网友热议战争风险与全球政治格局变化。\n",
    "\n",
    "社交媒体动态\n",
    "与此同时，国内社交媒体呈现两极分化：\n",
    "\n",
    "严肃讨论：部分用户转发央视新闻、专家解读，关注事件对经济的影响（如“黄金要不要现在买入？”）。\n",
    "\n",
    "娱乐化消解：另一部分网友以“二哈表情包”“哈哈哈”调侃紧张氛围，甚至将普京称为“雄狮”，形成“悲允”式黑色幽默。\n",
    "\n",
    "文化视角\n",
    "在文学社区，有读者推荐三叔的《落魄》全文，称其“值得一看”，并关联当前局势：“历史总是惊人相似，如同苏联解体时的沙姆事件。”\n",
    "\n",
    "编辑评述\n",
    "本次乌克兰危机不仅是地缘政治冲突，更是一场全球经济的压力测试。从市场反应看，能源与金融市场的波动揭示了全球化时代危机的传导性。值得注意的是，社交媒体上的分裂态度——严肃关注与娱乐化解构并存，反映了公众对复杂议题的多元应对策略。\n",
    "\n",
    "建议投资者：\n",
    "\n",
    "短期关注避险资产（黄金、美元），但警惕高位回调风险；\n",
    "\n",
    "长期需观察俄乌谈判进展及美联储政策变化。\n",
    "\n",
    "社会观察：当“普京”与“二哈”同屏出现时，我们或许该思考：娱乐化表达是缓解焦虑的方式，还是对严肃议题的消解？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"加载CSV文件并返回DataFrame\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"成功加载数据，共{len(df)}行，{len(df.columns)}列\")\n",
    "        print(f\"数据集中的列: {df.columns.tolist()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"加载数据时出错: {str(e)}\")\n",
    "        # 尝试其他编码\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='gbk')\n",
    "            print(f\"使用GBK编码成功加载数据，共{len(df)}行，{len(df.columns)}列\")\n",
    "            return df\n",
    "        except:\n",
    "            print(\"尝试使用GBK编码也失败了\")\n",
    "            return None\n",
    "\n",
    "def preprocess_text_column(df, text_column):\n",
    "    \"\"\"处理文本列，去除空值和重复值\"\"\"\n",
    "    if text_column not in df.columns:\n",
    "        print(f\"列 '{text_column}' 不在数据集中\")\n",
    "        return None\n",
    "    \n",
    "    # 删除空值\n",
    "    df = df.dropna(subset=[text_column])\n",
    "    # 删除重复值\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    print(f\"预处理后数据集大小: {len(df)}行\")\n",
    "    return df\n",
    "\n",
    "def analyze_text_content(texts):\n",
    "    \"\"\"分析文本内容，返回词频分析结果\"\"\"\n",
    "    # 使用jieba进行分词\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):  # 确保是字符串类型\n",
    "            words = jieba.cut(text)\n",
    "            all_words.extend(words)\n",
    "    \n",
    "    # 过滤停用词（常见的无意义词语）\n",
    "    stopwords = set(['的', '了', '在', '是', '我', '有', '和', '就', '不', '人', '都', '一', '一个', '上', '也', '很', '到', '说', '要', '去', '你', '会', '着', '没有', '看', '好', '自己', '这'])\n",
    "    filtered_words = [word for word in all_words if len(word) > 1 and word not in stopwords]\n",
    "    \n",
    "    # 统计词频\n",
    "    word_counts = Counter(filtered_words)\n",
    "    return word_counts\n",
    "\n",
    "def create_word_cloud(word_counts, output_file=\"wordcloud.png\"):\n",
    "    \"\"\"生成词云图\"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='simhei.ttf',  # 需要安装中文字体\n",
    "        width=800, \n",
    "        height=400,\n",
    "        background_color='white'\n",
    "    ).generate_from_frequencies(word_counts)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "    print(f\"词云图已保存为 {output_file}\")\n",
    "\n",
    "def topic_modeling(texts, n_topics=5):\n",
    "    \"\"\"使用LDA进行主题建模\"\"\"\n",
    "    # 创建文档-词频矩阵\n",
    "    vectorizer = CountVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # LDA模型\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    # 获取特征词\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # 每个主题的关键词\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-10-1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append((topic_idx, top_words))\n",
    "    \n",
    "    return topics\n",
    "\n",
    "def call_ai_for_summary(sample_texts):\n",
    "    \"\"\"调用AI生成媒体内容综述\"\"\"\n",
    "    # 为了演示，创建一个示例提示词\n",
    "    prompt = f\"\"\"\n",
    "    请对以下媒体内容样本进行综合分析并提供一份详细的综述，包括主题、情感倾向、风格特点、目标受众等:\n",
    "\n",
    "    {sample_texts[:5000]}  # 限制文本长度，避免超出API限制\n",
    "    \n",
    "    请提供:\n",
    "    1. 内容的主要主题和关键信息\n",
    "    2. 内容的风格和语调分析\n",
    "    3. 潜在的受众群体\n",
    "    4. 内容的意图和目的分析\n",
    "    5. 内容质量和专业程度的评估\n",
    "    \"\"\"\n",
    "    \n",
    "    # 实际项目中，你需要使用类似OpenAI API的接口\n",
    "    # 这里用模拟的方式展示\n",
    "    print(\"\\nAI分析提示词:\")\n",
    "    print(prompt)\n",
    "    \n",
    "    # 模拟API返回内容\n",
    "    ai_response = \"\"\"\n",
    "    ## 媒体内容综述分析\n",
    "\n",
    "    ### 主要主题与关键信息\n",
    "    根据分析的样本内容，此媒体内容主要围绕社交媒体、新闻报道和娱乐资讯展开。关键信息包括时事新闻、社会热点话题以及一些娱乐圈动态。内容覆盖面较广，从政治经济到日常生活话题均有涉及。\n",
    "\n",
    "    ### 风格与语调分析\n",
    "    内容风格以客观叙述为主，但也包含一定的评论性质。语调整体上偏向正式，但在娱乐相关内容中也出现了较为轻松活泼的表达方式。文本多使用陈述句和说明性语言，专业术语与通俗表达并存。\n",
    "\n",
    "    ### 潜在受众群体\n",
    "    目标受众主要为中等教育水平以上的成年人，特别是对时事新闻、社会话题有关注的人群。从内容的多样性来看，受众年龄段可能较广，但核心群体应为25-45岁的城市居民。\n",
    "\n",
    "    ### 内容意图与目的\n",
    "    该媒体内容旨在提供信息、引发思考并在一定程度上引导舆论。部分内容带有明显的观点传递目的，意在影响读者对特定事件或现象的看法。同时，内容也包含了知识普及和娱乐消遣的功能。\n",
    "\n",
    "    ### 内容质量与专业程度\n",
    "    整体内容质量中上，信息传递较为清晰，但专业深度有所不足。部分内容存在信息密度低、重复性强的问题。在专业性方面，内容更侧重于通俗化表达，专业术语使用适度，适合大众阅读理解。\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": ai_response\n",
    "    }\n",
    "\n",
    "def generate_comprehensive_report(df, text_column, word_counts, topics, ai_results):\n",
    "    \"\"\"生成综合报告\"\"\"\n",
    "    top_words = word_counts.most_common(20)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "    # 媒体内容综合分析报告\n",
    "    \n",
    "    ## 数据集概况\n",
    "    - 总条目数: {len(df)}\n",
    "    - 分析的列: '{text_column}'\n",
    "    \n",
    "    ## 词频分析\n",
    "    最常见的20个词语:\n",
    "    {', '.join([f'{word}({count})' for word, count in top_words])}\n",
    "    \n",
    "    ## 主题分析\n",
    "    {len(topics)}个识别出的主题:\n",
    "    \"\"\"\n",
    "    \n",
    "    for topic_idx, words in topics:\n",
    "        report += f\"\\n主题 {topic_idx+1}: {', '.join(words)}\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "    ## AI生成的内容综述\n",
    "    {ai_results['response']}\n",
    "    \n",
    "    ## 分析方法\n",
    "    本分析采用以下步骤进行:\n",
    "    1. 数据加载与预处理\n",
    "    2. 文本分词与词频统计 \n",
    "    3. 词云图可视化\n",
    "    4. 主题建模分析\n",
    "    5. AI辅助内容综述\n",
    "    \n",
    "    ## 使用的工具\n",
    "    - Python和pandas进行数据处理\n",
    "    - jieba进行中文分词\n",
    "    - WordCloud生成词云图\n",
    "    - scikit-learn进行主题建模\n",
    "    - AI大语言模型进行内容综述\n",
    "    \"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数，协调整个分析流程\"\"\"\n",
    "    # 定义文件路径\n",
    "    file_path = \"data/doc_ewujushi.csv\"\n",
    "    \n",
    "    # 步骤1: 加载数据\n",
    "    print(\"步骤1: 加载数据\")\n",
    "    df = load_data(file_path)\n",
    "    if df is None:\n",
    "        print(\"数据加载失败，退出程序。\")\n",
    "        return\n",
    "    \n",
    "    # 步骤2: 识别和预处理文本列\n",
    "    # 假设第一个字符串类型的列是文本列\n",
    "    text_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    if not text_columns:\n",
    "        print(\"未找到文本列，退出程序。\")\n",
    "        return\n",
    "    \n",
    "    text_column = text_columns[0]  # 使用第一个文本列\n",
    "    print(f\"步骤2: 预处理文本列 '{text_column}'\")\n",
    "    processed_df = preprocess_text_column(df, text_column)\n",
    "    if processed_df is None:\n",
    "        return\n",
    "    \n",
    "    # 步骤3: 分析文本内容\n",
    "    print(\"步骤3: 分析文本内容\")\n",
    "    texts = processed_df[text_column].tolist()\n",
    "    word_counts = analyze_text_content(texts)\n",
    "    \n",
    "    # 步骤4: 可视化 - 生成词云\n",
    "    print(\"步骤4: 生成词云图\")\n",
    "    create_word_cloud(word_counts)\n",
    "    \n",
    "    # 步骤5: 主题建模\n",
    "    print(\"步骤5: 主题建模\")\n",
    "    # 确保所有文本都是字符串类型\n",
    "    clean_texts = [str(text) for text in texts if isinstance(text, str)]\n",
    "    topics = topic_modeling(clean_texts)\n",
    "    \n",
    "    # 步骤6: 调用AI进行内容综述\n",
    "    print(\"步骤6: 调用AI生成综述\")\n",
    "    sample_texts = \"\\n\".join(clean_texts[:50])  # 取前50条文本作为样本\n",
    "    ai_results = call_ai_for_summary(sample_texts)\n",
    "    \n",
    "    # 步骤7: 生成综合报告\n",
    "    print(\"步骤7: 生成综合报告\")\n",
    "    report = generate_comprehensive_report(processed_df, text_column, word_counts, topics, ai_results)\n",
    "    \n",
    "    print(\"\\n最终分析报告:\")\n",
    "    print(report)\n",
    "    \n",
    "    # 将报告保存到文件\n",
    "    with open(\"media_content_analysis_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    print(\"\\n报告已保存到 media_content_analysis_report.md\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result\n",
    "\n",
    "# 媒体内容综合分析报告\n",
    "    \n",
    "## 数据集概况\n",
    "- 总条目数: 25685\n",
    "- 分析的列: 'author'\n",
    "    \n",
    "## 词频分析\n",
    "最常见的20个词语:\n",
    "用户(144), 财经(114), 微博(111), 先生(98), 世界(90), 快乐(83), --(82), 投资(81), __(72), 不是(68), 123(67), 就是(66), 小小(65), 努力(65), 今天(64), 一只(59), 名字(57), 股市(56), 昵称(56), 人生(55)\n",
    "    \n",
    "## 主题分析\n",
    "5个识别出的主题:\n",
    "    \n",
    "主题 1: mr, 01, tuk, ink, only, 77, al, 先生, tony, 清风\n",
    "\n",
    "主题 2: 001, why, jay, 007, wu, ll, leo, 狮子大叔, li, xi\n",
    "\n",
    "主题 3: 财经达人, 天津, super, ii, 2013, 1_1, a股, william, 2016, 控盘猫\n",
    "\n",
    "主题 4: ly, max, the, in, liang, han, boy, 财经, dawn, haitao\n",
    "\n",
    "主题 5: 招财, 2020, 兜里个兜, 777, ic, pro, 大道至简, 凤凰卫视, pan, chen\n",
    "\n",
    "## AI生成的内容综述\n",
    "    \n",
    "## 媒体内容综述分析\n",
    "\n",
    "### 主要主题与关键信息\n",
    "根据分析的样本内容，此媒体内容主要围绕社交媒体、新闻报道和娱乐资讯展开。关键信息包括时事新闻、社会热点话题以及一些娱乐圈动态。内容覆盖面较广，从政治经济到日常生活话题均有涉及。\n",
    "\n",
    "### 风格与语调分析\n",
    "内容风格以客观叙述为主，但也包含一定的评论性质。语调整体上偏向正式，但在娱乐相关内容中也出现了较为轻松活泼的表达方式。文本多使用陈述句和说明性语言，专业术语与通俗表达并存。\n",
    "\n",
    "### 潜在受众群体\n",
    "目标受众主要为中等教育水平以上的成年人，特别是对时事新闻、社会话题有关注的人群。从内容的多样性来看，受众年龄段可能较广，但核心群体应为25-45岁的城市居民。\n",
    "\n",
    "### 内容意图与目的\n",
    "该媒体内容旨在提供信息、引发思考并在一定程度上引导舆论。部分内容带有明显的观点传递目的，意在影响读者对特定事件或现象的看法。同时，内容也包含了知识普及和娱乐消遣的功能。\n",
    "\n",
    "### 内容质量与专业程度\n",
    "整体内容质量中上，信息传递较为清晰，但专业深度有所不足。部分内容存在信息密度低、重复性强的问题。在专业性方面，内容更侧重于通俗化表达，专业术语使用适度，适合大众阅读理解。\n",
    "    \n",
    "    \n",
    "## 分析方法\n",
    "本分析采用以下步骤进行:\n",
    "1. 数据加载与预处理\n",
    "2. 文本分词与词频统计 \n",
    "3. 词云图可视化\n",
    "4. 主题建模分析\n",
    "5. AI辅助内容综述\n",
    "    \n",
    "## 使用的工具\n",
    "- Python和pandas进行数据处理\n",
    "- jieba进行中文分词\n",
    "- WordCloud生成词云图\n",
    "- scikit-learn进行主题建模\n",
    "- AI大语言模型进行内容综述\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
