{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五次课后练习 之一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**负责助教：朱轩宇**\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">请将作业文件命名为 第五次课后练习+姓名+学号.ipynb, 例如 第五次课后练习+张三+1000000000.ipynb</span>\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">在作业过程中觉得有心得或者自己拓展学习到有价值内容的，可以在文件名最后加一个#号。例如第五次课后练习+张三+1000000000+#.ipynb</span>\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">本次课同时发布课后练习和选做题，提交时请注意区分提交通道</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第零部分 代码理解\n",
    "\n",
    "请认真阅读代码，理解代码的功能，先写出预想的结果。运行并检验结果是否如预期。如果不如预期，请分析理解其中的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.1** 多进程编程，进程池，进程间通讯\n",
    "    阅读理解下面代码，观察四次运行的结果，解释出现这个结果的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这段代码使用了multiprocessing模块来实现多进程任务处理。主要功能是通过多个工作进程（CustomWorker）从任务队列中获取任务，处理任务后将结果放入结果队列。主进程负责管理任务队列、结果队列以及进程池的创建和销毁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multiprocessing_script.py\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "#`Manager`类是Python `multiprocessing`模块中的一个工具，用于创建可以在多个进程之间\n",
    "#共享的对象（如列表、字典、队列等），简化了进程间数据共享和通信的复杂性。\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "# 该类封装了工作进程的逻辑，包括任务处理、结果返回等。\n",
    "# run方法是一个无限循环，从任务队列中获取任务并处理，直到接收到TERMINATE信号。\n",
    "class CustomWorker:\n",
    "    def __init__(self, worker_id, task_queue, result_queue, config):\n",
    "        self.worker_id = worker_id\n",
    "        self.task_queue = task_queue\n",
    "        self.result_queue = result_queue\n",
    "        self.__secret_key = config['key']\n",
    "        self.__mode = config['mode']\n",
    "\n",
    "    def run(self):\n",
    "        # print(f\"Worker {self.worker_id} started with mode {self.__mode}\")\n",
    "        while True:\n",
    "            try:\n",
    "                task = self.task_queue.get()\n",
    "                if task == 'TERMINATE':\n",
    "                    # print(f\"Worker {self.worker_id} terminating\")\n",
    "                    break\n",
    "                result = self.__process(task)\n",
    "                self.result_queue.put({'worker': self.worker_id, 'result': result, 'task': task})\n",
    "            except Exception as e:\n",
    "                self.result_queue.put({'worker': self.worker_id, 'error': str(e), 'task': task})\n",
    "\n",
    "    # 会根据具体每个worker实例的mode对输入的task做不同的操作\n",
    "    def __process(self, task):\n",
    "        if self.__mode == 'encrypt':\n",
    "            return f\"{task}_{self.__secret_key}\"   # 拼接输入的密码\n",
    "        elif self.__mode == 'hash':\n",
    "            return hash(task + self.__secret_key)  # hash输入的密码\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode\")\n",
    "\n",
    "# 创建CustomWorker实例并调用其run方法。\n",
    "# 每个工作进程根据其ID的奇偶生成不同的配置（encrypt和hash）。\n",
    "def worker_process(task_queue, result_queue):\n",
    "    worker_id = multiprocessing.current_process()._identity[0]\n",
    "    config = {\n",
    "        'key': f\"KEY{worker_id}\",\n",
    "        'mode': 'encrypt' if worker_id % 2 == 0 else 'hash'\n",
    "    }\n",
    "    worker = CustomWorker(worker_id, task_queue, result_queue, config)\n",
    "    worker.run()  \n",
    "\n",
    "def main():\n",
    "    manager = Manager()\n",
    "    task_queue = manager.Queue()    # 任务队列\n",
    "    result_queue = manager.Queue()  # 结果队列\n",
    "\n",
    "    pool_size = 2  # 进程池大小\n",
    "     \n",
    "    expected_tasks = 5  # 期望处理的任务数\n",
    "    tasks_processed = 0\n",
    "    \n",
    "    # 收集处理结果\n",
    "    results = []\n",
    "    \n",
    "    # 在任务队列里先放入所需要加工的任务\n",
    "    for i in range( expected_tasks):\n",
    "        task_queue.put(f\"task_{i}\")\n",
    "    \n",
    "    # 在任务队列后添加足够的终止信号（每个进程会消耗掉一个）。这里可以用multiprocessing.Event代替更好\n",
    "    # 这个方式某种情况下可能导致进程无法终结的问题？\n",
    "    for _ in range(pool_size):\n",
    "        task_queue.put(\"TERMINATE\")\n",
    "\n",
    "    # 创建并启动进程池\n",
    "    pool = Pool(\n",
    "        processes = pool_size,\n",
    "        initializer = worker_process,\n",
    "        initargs = (task_queue, result_queue)\n",
    "    )\n",
    "    \n",
    "    while tasks_processed < expected_tasks:\n",
    "        result = result_queue.get()\n",
    "        tasks_processed += 1\n",
    "        \n",
    "        if 'error' in result:\n",
    "            print(f\"Error from worker {result['worker']} processing {result['task']}: {result['error']}\")\n",
    "        else:\n",
    "            results.append(result)\n",
    "            # print(f\"Worker {result['worker']} processed {result['task']} → {result['result']}\")\n",
    "    \n",
    "    # 等待所有进程完成\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # print(f\"Processed {len(results)} tasks successfully\")\n",
    "    print(f\"Final results: {results}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"Running subprocess {i}\")\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"multiprocessing_script.py\"], \n",
    "        capture_output=True, text=True)\n",
    "    print(\"STDOUT:\", result.stdout)  # 查看任务处理结果\n",
    "    print(\" \")\n",
    "    print(\"STDERR:\", result.stderr)  # 查看是否有错误\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预期结果为\n",
    "```bash\n",
    "Running subprocess 0\n",
    "STDOUT: Final results: [{'worker': 2, 'result': 'task_0_KEY2', 'task': 'task_0'}, {'worker': 3, 'result': 4643786775758808515, 'task': 'task_1'}, {'worker': 3, 'result': -6929853957667505824, 'task': 'task_3'}, {'worker': 2, 'result': 'task_2_KEY2', 'task': 'task_2'}, {'worker': 3, 'result': -3414338737549556841, 'task': 'task_4'}]\n",
    "\n",
    " \n",
    "STDERR: \n",
    " \n",
    "Running subprocess 1\n",
    "STDOUT: Final results: [{'worker': 2, 'result': 'task_0_KEY2', 'task': 'task_0'}, {'worker': 2, 'result': 'task_1_KEY2', 'task': 'task_1'}, {'worker': 3, 'result': 8771837200597741918, 'task': 'task_2'}, {'worker': 2, 'result': 'task_3_KEY2', 'task': 'task_3'}, {'worker': 3, 'result': -9086075438850061251, 'task': 'task_4'}]\n",
    "\n",
    " \n",
    "STDERR: \n",
    " \n",
    "Running subprocess 2\n",
    "STDOUT: Final results: [{'worker': 2, 'result': 'task_0_KEY2', 'task': 'task_0'}, {'worker': 2, 'result': 'task_1_KEY2', 'task': 'task_1'}, {'worker': 2, 'result': 'task_2_KEY2', 'task': 'task_2'}, {'worker': 2, 'result': 'task_3_KEY2', 'task': 'task_3'}, {'worker': 2, 'result': 'task_4_KEY2', 'task': 'task_4'}]\n",
    "\n",
    " \n",
    "STDERR: \n",
    " \n",
    "Running subprocess 3\n",
    "STDOUT: Final results: [{'worker': 3, 'result': 7845306417593060954, 'task': 'task_0'}, {'worker': 2, 'result': 'task_1_KEY2', 'task': 'task_1'}, {'worker': 3, 'result': -8272236163030477316, 'task': 'task_2'}, {'worker': 2, 'result': 'task_3_KEY2', 'task': 'task_3'}, {'worker': 3, 'result': 8804919445247845600, 'task': 'task_4'}]\n",
    "\n",
    " \n",
    "STDERR:\n",
    "```\n",
    "\n",
    "实际与预期相符 (实际数字可能与上面不符). 这些运行的结果中, 工作模式2与工作模式3绝大多数时候都是共同处理任务, 但是也有工作进程2碰巧在工作进程3获得任何任务之前, 获取并处理了所有5个任务, 这是并发处理可能存在的正常情况. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.2** 协程\n",
    "    阅读理解下面代码，观察运行的结果，解释结果的原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " \n",
    "#定义一个消费者，他有名字name\n",
    "#因为里面有yield，本质上是一个生成器\n",
    "def consumer(name): \n",
    "    print(f'{name}  准备吃包子啦！,呼吁店小二')\n",
    "    while True:\n",
    "        baozi=yield  #接收send传的值，并将值赋值给变量baozi\n",
    "        print(f'包子 {baozi+1} 来了,被 {name} 吃了！')\n",
    " \n",
    "#定义一个生产者，生产包子的店家，店家有一个名字name,并且有两个顾客c1 c2\n",
    "def producer(name,c1,c2):\n",
    "    next(c1)  #启动生成器c1\n",
    "    next(c2)  #启动生成器c2\n",
    "    print(f'{name} 开始准备做包子啦！')\n",
    "    for i in range(3):\n",
    "        time.sleep(1)\n",
    "        print(f'做了第{i+1}包子，分成两半,你们一人一半')\n",
    "        c1.send(i)\n",
    "        c2.send(i)\n",
    "        print('------------------------------------')\n",
    " \n",
    "c1=consumer('张三') #把函数变成一个生成器\n",
    "c2=consumer('李四')\n",
    "producer('店小二',c1,c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预期结果为\n",
    "```bash\n",
    "张三  准备吃包子啦！,呼吁店小二\n",
    "李四  准备吃包子啦！,呼吁店小二\n",
    "店小二 开始准备做包子啦！\n",
    "做了第1包子，分成两半,你们一人一半\n",
    "包子 1 来了,被 张三 吃了！\n",
    "包子 1 来了,被 李四 吃了！\n",
    "------------------------------------\n",
    "做了第2包子，分成两半,你们一人一半\n",
    "包子 2 来了,被 张三 吃了！\n",
    "包子 2 来了,被 李四 吃了！\n",
    "------------------------------------\n",
    "做了第3包子，分成两半,你们一人一半\n",
    "包子 3 来了,被 张三 吃了！\n",
    "包子 3 来了,被 李四 吃了！\n",
    "------------------------------------\n",
    "```\n",
    "\n",
    "实际与预期相符. 这是一个经典的生产者-消费者问题, `consumer`函数不断接收`send`函数传来的内容，并打印\"包子 {baozi+1} 来了,被 {name} 吃了！\", 同时每生成一个包子就打印\"做了第{i+1}包子，分成两半,你们一人一半\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.3** 正则表达式、网路编程和任务调度\n",
    "\n",
    "    当运行此程序时，最终会输出哪些任务执行信息？说明具体输出顺序和原因\n",
    "\n",
    "    解释正则表达式^(\\d+):(.+)$的作用及其在代码中的具体应用场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import socket\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "'''\n",
    "TaskScheduler 类\n",
    "功能：实现一个简单的任务调度器，支持添加延迟任务并按时执行。\n",
    "\n",
    "关键点：\n",
    "使用线程锁 (threading.Lock) 保证任务列表的线程安全。\n",
    "使用 datetime 和 timedelta 计算任务的执行时间。\n",
    "使用后台线程 (daemon=True) 持续检查并执行到期的任务。\n",
    "'''\n",
    "           \n",
    "class TaskScheduler:\n",
    "    def __init__(self):\n",
    "        self.tasks = []  # 存储任务列表，每个任务是一个元组 (执行时间, 命令)\n",
    "        self.lock = threading.Lock()  # 线程锁，用于保护任务列表的线程安全\n",
    "    \n",
    "    def add_task(self, delay, command):\n",
    "        with self.lock:  # 加锁，确保线程安全\n",
    "            execute_time = datetime.now() + timedelta(seconds=delay)  # 计算任务的执行时间\n",
    "            self.tasks.append((execute_time, command))  # 将任务添加到任务列表\n",
    "            print(f\"Scheduled: {command} at {execute_time.strftime('%H:%M:%S')}\")  # 打印任务调度信息\n",
    "    \n",
    "    def start(self):\n",
    "        threading.Thread(target=self._run, daemon=True).start()  # 启动后台线程运行任务调度器\n",
    "    \n",
    "    def _run(self):\n",
    "        while True:  # 持续运行(提供服务)\n",
    "            now = datetime.now()  # 获取当前时间\n",
    "            with self.lock:  # 加锁，确保线程安全\n",
    "                due_tasks = [(t, cmd) for t, cmd in self.tasks if t <= now]  # 筛选出到期的任务\n",
    "                self.tasks = [(t, cmd) for t, cmd in self.tasks if t > now]  # 更新任务列表，移除已到期的任务\n",
    "            \n",
    "            for task in due_tasks:  # 遍历并执行到期的任务\n",
    "                print(f\"Executing: {task[1]} at {now.strftime('%H:%M:%S')}\")  # 打印任务执行信息\n",
    "                time.sleep(1)  # 模拟任务执行时间\n",
    "            \n",
    "            time.sleep(0.5)  # 每隔 0.5 秒检查一次任务列表\n",
    "\n",
    "            \n",
    "'''\n",
    "handle_client 函数\n",
    "功能：处理客户端连接，解析客户端发送的命令并添加到任务调度器中。\n",
    "\n",
    "关键点：\n",
    "使用正则表达式 (re.compile) 解析客户端发送的命令。\n",
    "具体命令格式为 ? \n",
    "将解析后的任务添加到任务调度器中。\n",
    "'''\n",
    "def handle_client(conn, scheduler):\n",
    "    pattern = re.compile(r'^(\\d+):(.+)$')  # 正则表达式，匹配具体\"？内容\"\n",
    "    while True:\n",
    "        data = conn.recv(1024).decode()  # 接收客户端发送的数据\n",
    "        if not data:  # 如果数据为空，断开连接\n",
    "            break\n",
    "        match = pattern.match(data.strip())  # 使用正则表达式匹配数据\n",
    "        if match:\n",
    "            delay = int(match.group(1))  # 提取内容1\n",
    "            cmd = match.group(2)  # 提取内容2\n",
    "            scheduler.add_task(delay, cmd)  # 将任务添加到调度器\n",
    "        else:\n",
    "            print(f\"Invalid command: {data}\")  # 如果命令格式无效，打印错误信息\n",
    "\n",
    "'''\n",
    "server 函数\n",
    "功能：启动服务器，监听客户端连接并为每个客户端创建一个线程处理请求。\n",
    "\n",
    "关键点：\n",
    "使用 socket 创建 TCP 服务器。\n",
    "为每个客户端连接创建一个新线程，调用 handle_client 处理客户端请求。\n",
    "'''\n",
    "def server():\n",
    "    scheduler = TaskScheduler()  # 创建任务调度器实例\n",
    "    scheduler.start()  # 启动任务调度器\n",
    "    \n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:  # 创建 TCP 套接字\n",
    "        s.bind(('localhost', 65432))  # 绑定到本地地址和端口\n",
    "        s.listen()  # 开始监听连接\n",
    "        print(\"Server started\")  # 打印服务器启动信息\n",
    "        while True:\n",
    "            conn, addr = s.accept()  # 接受客户端连接\n",
    "            print(f\"Connected by {addr}\")  # 打印客户端地址信息\n",
    "            threading.Thread(target=handle_client, args=(conn, scheduler)).start()  # 为每个客户端创建新线程\n",
    "\n",
    "'''\n",
    "4. 主程序\n",
    "功能：启动服务器并模拟客户端发送任务。\n",
    "\n",
    "关键点：\n",
    "使用 threading.Thread 启动服务器线程。\n",
    "模拟客户端发送任务到服务器。\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    threading.Thread(target=server, daemon=True).start()  # 启动服务器线程\n",
    "    time.sleep(1)  # 等待服务器启动\n",
    "    \n",
    "    # 模拟客户端发送任务\n",
    "    def client(msg):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:  # 创建客户端套接字\n",
    "            s.connect(('localhost', 65432))  # 连接到服务器\n",
    "            s.sendall(msg.encode())  # 发送任务消息\n",
    "    \n",
    "    client(\"3:Task1\")  # 发送任务 1，\n",
    "    time.sleep(1)\n",
    "    client(\"2:Task2\")  # 发送任务 2，\n",
    "    client(\"1:Task3\")  # 发送任务 3，\n",
    "    \n",
    "    time.sleep(10)  # 保持主线程运行，确保任务执行完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行任务的顺序应该为 Task 3, Task 1 & Task 2. 在发送 Task 1 和 Task 2 之间有大约 1s 的间隔, 而 Task 2 和 Task 3 之间的间隔应当会小于 1s, 因此, 加上延迟时间的话 Task 3 应该最先执行, 随后由于Task1 和 Task2 同时满足\"到期\"条件, 由于任务列表中保存的顺序是先添加的 Task1, 再添加的 Task2 (且在 `_run()` 中使用了列表解析一次性筛选), 这时 for 循环会先执行 Task1, 再执行 Task2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.4** 网络爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    阅读下面代码，解释输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                # 用于发送HTTP请求\n",
    "from bs4 import BeautifulSoup  # 用于解析HTML文档\n",
    "import re                      # 用于正则表达式操作\n",
    "import threading               # 用于多线程编程\n",
    "\n",
    "# 定义需要爬取的URL列表\n",
    "urls = [\"https://www.pku.edu.cn/\", \"https://its.pku.edu.cn/\", \"https://eecs.pku.edu.cn/\"]\n",
    "\n",
    "# 定义爬取函数\n",
    "def crawl(url):\n",
    "    print(f\"Crawling {url}\")  # 打印当前正在爬取的URL\n",
    "    response = requests.get(url)  # 发送HTTP GET请求，获取网页内容\n",
    "    response.encoding = response.apparent_encoding  # 根据网页内容自动设置编码\n",
    "    html = response.text  # 获取网页的HTML内容\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')  # 使用BeautifulSoup解析HTML文档\n",
    "\n",
    "    cookies = response.cookies  # 获取服务器返回的Cookies\n",
    "    print(cookies)  # 打印Cookies\n",
    "\n",
    "    # 将Cookies写入文件\n",
    "    with open('cookies.txt', 'a') as f:  # 以追加模式打开文件\n",
    "        f.write(url + ': ' + str(cookies) + '\\n')  # 将URL和对应的Cookies写入文件\n",
    "\n",
    "    # 提取网页中的纯文本内容，并去除多余的空格\n",
    "    text = re.sub(r'\\s+', ' ', soup.get_text())  # 使用正则表达式具体做了？\n",
    "    print(text)  # 打印提取的文本内容\n",
    "\n",
    "    # 将提取的文本内容写入文件\n",
    "    with open('text.txt', 'a') as f:  # 以追加模式打开文件\n",
    "        f.write(text + '\\n')  # 将文本内容写入文件\n",
    "\n",
    "# 创建线程列表，每个线程负责爬取一个URL\n",
    "threads = [threading.Thread(target=crawl, args=(url, )) for url in urls]\n",
    "\n",
    "# 启动所有线程\n",
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "# 等待所有线程完成\n",
    "for t in threads:\n",
    "    t.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出分为两部分, 一个是进行爬虫, 另一个是进行文本写入, 其中对于爬虫部分, 先爬取URL打印信息, 随后输出Cookies, 再利用正则表达式使得提取的网页纯文本内容变得更加整洁, 避免由于多余的换行或空格造成的排版混乱, 将Cookies写入cookies.txt文件, 将提取的文本写入text.txt文件. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分 基础练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1** 进程的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multiprocessing_script_task1_1.py\n",
    "#导入模块\n",
    "import multiprocessing\n",
    "import time\n",
    " \n",
    "#创建进程调用函数\n",
    "def work1(interval):\n",
    "\tprint('执行work1')\n",
    "\ttime.sleep(interval)\n",
    "\tprint('end work1')\n",
    " \n",
    "def work2(interval):\n",
    "\tprint('执行work2')\n",
    "\ttime.sleep(interval)\n",
    "\tprint('end work2')\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "\tprint('执行主进程')\n",
    "\t# 创建进程对象\n",
    "\tp1 = multiprocessing.Process(target=work1,args=(2,))\n",
    "\tp2 = multiprocessing.Process(target=work2,args=(3,))\n",
    "\t# 启动进程\n",
    "\tp1.start()\n",
    "\tp2.start()\n",
    "\tp1.join()\n",
    "\tp2.join()\n",
    "\tprint('主进程结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"multiprocessing_script_task1_1.py\"], \n",
    "    capture_output=True, text=True)\n",
    "assert \"执行主进程\" in result.stdout\n",
    "assert \"执行work1\" in result.stdout\n",
    "assert \"执行work2\" in result.stdout\n",
    "assert \"end work1\" in result.stdout\n",
    "assert \"end work2\" in result.stdout\n",
    "assert \"主进程结束\" in result.stdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2** 进程池"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multiprocessing_script_task1_2.py\n",
    "import multiprocessing\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 代码填空：创建一个进程池，进程数为4\n",
    "    with multiprocessing.Pool(4) as pool:\n",
    "        results = [pool.apply_async(square, (i,)) for i in range(5)]\n",
    "        output = [res.get() for res in results]\n",
    "    print(output)  # 应该输出 [0, 1, 4, 9, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"multiprocessing_script_task1_2.py\"], \n",
    "    capture_output=True, text=True)\n",
    "assert result.stdout == \"[0, 1, 4, 9, 16]\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3** 进程消息传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multiprocessing_script_task1_3.py\n",
    "import multiprocessing\n",
    "\n",
    "def consumer(q):\n",
    "    while True:\n",
    "        # 从队列中获取数据\n",
    "        item = q.get()\n",
    "        if item == 'STOP':\n",
    "            break\n",
    "        print(f\"消费: {item}\")\n",
    "\n",
    "def producer(q):\n",
    "    for i in range(3):\n",
    "        # 代码填空：向队列中添加数据，内容为\"产品i\"\n",
    "        q.put(f\"产品{i}\")\n",
    "    q.put('STOP')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 创建一个队列\n",
    "    q = multiprocessing.Queue()\n",
    "    p1 = multiprocessing.Process(target=producer, args=(q,))\n",
    "    p2 = multiprocessing.Process(target=consumer, args=(q,))\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p1.join()\n",
    "    p2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"multiprocessing_script_task1_3.py\"], \n",
    "    capture_output=True, text=True)\n",
    "# print(result.stdout)\n",
    "assert \"消费: 产品0\" in result.stdout\n",
    "assert \"消费: 产品1\" in result.stdout\n",
    "assert \"消费: 产品2\" in result.stdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4** 线程的创建和传参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建两个线程，分别计算`x`, `y`的和与积，存入`result`中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "x, y = 3, 4\n",
    "\n",
    "result = {}\n",
    "def worker1(x, y):\n",
    "    result['add'] = x + y\n",
    "    \n",
    "def worker2(x, y):\n",
    "    result['multiply'] = x * y\n",
    "\n",
    "t1 = threading.Thread(target=worker1, args=(x, y)) # create a thread\n",
    "t2 = threading.Thread(target=worker2, args=(x, y)) # create a thread\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "assert result == {'add': 7, 'multiply': 12}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5** 线程锁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以下代码用`#######`包含的部分中，在合适的位置添加和应用线程锁，保证`counter`线程安全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "\n",
    "counter = []\n",
    "lock = threading.Lock()  # 创建锁对象\n",
    "\n",
    "##########################################\n",
    "# add and apply a lock\n",
    "def worker():\n",
    "    for _ in range(100):\n",
    "        with lock:\n",
    "            counter.append(1)\n",
    "            idx = len(counter) - 1\n",
    "        time.sleep(random.random() * 0.01)\n",
    "        with lock:\n",
    "            counter[idx] += 1\n",
    "##########################################\n",
    "\n",
    "threads = [threading.Thread(target=worker) for _ in range(10)]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "assert counter == [2] * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.6** 生成器与yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coroutine_example():\n",
    "    value = yield 0\n",
    "    print(f'Received value: {value}')\n",
    "    assert value == 3\n",
    "    value = yield 1\n",
    "    print(f'Received value: {value}')\n",
    "    # 返回5\n",
    "    yield 5\n",
    "\n",
    "c = coroutine_example()\n",
    "next(c)\n",
    "# 发送3\n",
    "c.send(3)\n",
    "assert c.send(4) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.7** 基于协程求平均数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average():\n",
    "    total = 0.0  #数字的总和\n",
    "    count = 0    #数字的个数\n",
    "    avg = None   #平均值\n",
    "    while True:\n",
    "        ##############################\n",
    "        # 接收一个数字，并实现计算平均值的逻辑\n",
    "        num = yield avg      # 接收一个数值\n",
    "        total += num         # 更新总和\n",
    "        count += 1           # 更新计数\n",
    "        avg = total / count  # 计算平均值\n",
    "        ##############################\n",
    " \n",
    "#定义一个函数，通过这个函数向average函数发送数值\n",
    "def sender(generator):\n",
    "    print(next(generator))  #启动生成器\n",
    "    assert generator.send(10)==10  # 10\n",
    "    assert generator.send(20)==15  # 15\n",
    "    assert generator.send(30)==20  # 20\n",
    "    assert generator.send(40)==25  # 25\n",
    " \n",
    " \n",
    "g = average()\n",
    "sender(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.8** 正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"My phone number is 13345678901 and 15592883746\"\n",
    "# 定义正则表达式，匹配手机号码\n",
    "pattern = r'1[3-9]\\d{9}'\n",
    "matches = re.findall(pattern, text)  \n",
    "assert matches == ['13345678901', '15592883746']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.9** 网络编程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile socket_server.py\n",
    "# 服务端\n",
    "import socket\n",
    "\n",
    "# 创建服务器套接字，绑定到本地地址，端口9999，监听连接，最大连接数为5\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind(('localhost', 9999))\n",
    "server_socket.listen(5)\n",
    "\n",
    "print(\"服务器已启动，等待连接...\")\n",
    "\n",
    "while True:\n",
    "    # 接受客户端连接\n",
    "    client_socket, addr = server_socket.accept()\n",
    "    print(f\"接收到来自 {addr} 的连接\")\n",
    "    \n",
    "    # 发送欢迎消息\n",
    "    message = \"欢迎！\"\n",
    "    client_socket.sendall(message.encode())\n",
    "    \n",
    "    # 关闭连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile socket_client.py\n",
    "# 客户端\n",
    "import socket\n",
    "\n",
    "# 创建客户端套接字\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# 连接到服务器\n",
    "client_socket.connect(('localhost', 9999))\n",
    "\n",
    "# 接收欢迎消息\n",
    "message = client_socket.recv(1024).decode()\n",
    "print(message)\n",
    "\n",
    "# 关闭连接\n",
    "client_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# 启动服务器，正确捕获输出\n",
    "server = subprocess.Popen(\n",
    "    [\"python\", \"socket_server.py\"], \n",
    "    stdout=subprocess.PIPE, \n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# 给服务器一些启动时间\n",
    "time.sleep(1)\n",
    "\n",
    "# 运行客户端\n",
    "client = subprocess.run(\n",
    "    [\"python\", \"socket_client.py\"], \n",
    "    capture_output=True, \n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(client.stdout)\n",
    "server.kill()\n",
    "assert \"欢迎！\" in client.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分 进阶练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1** 正则表达式提取参考文献信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    运用正则表达式从下面参考文献中提取作者列表，文章名称，发表时间。\n",
    "    结果存为JSON格式文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "1. Agarwal, Nitin, Ravi Shankar Reddy, Kiran Gvr, and Carolyn Penstein Rosé. 2011. \"Towards multi-document summarization of scientific articles: making interesting comparisons with SciSumm.\" In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 8-15, Portland, Oregon. Association for Computational Linguistics.\n",
    "\n",
    "2. Beltagy, Iz, Kyle Lo, and Arman Cohan. 2019. \"SciBERT: A pretrained language model for scientific text.\" In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615-3620, Hong Kong, China. Association for Computational Linguistics.\n",
    "\n",
    "3. Beltagy, Iz, Matthew E Peters, and Arman Cohan. 2020. \"Longformer: The long-document transformer.\" arXiv preprint arXiv:2004.05150.\n",
    "\n",
    "4. Bornmann, Lutz, and Rüdiger Mutz. 2015. \"Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references.\" Journal of the Association for Information Science and Technology, 66(11): 2215-2222.\n",
    "\n",
    "5. Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. \"Language models are few-shot learners.\" In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901.\n",
    "\n",
    "6. Cohan, Arman, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. \"A discourse-aware attention model for abstractive summarization of long documents.\" In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computational Linguistics.\n",
    "\n",
    "7. Cohan, Arman, Guy Feigenblat, Dayne Freitag, Tirthankar Ghosal, Drahomira Herrmannova, Petr Knoth, Kyle Lo, Philipp Mayr, Michal Shmueli-Scheuer, Anita de Waard, and Lucy Lu Wang. 2022. \"Overview of the third workshop on scholarly document processing.\" In Proceedings of the Third Workshop on Scholarly Document Processing, pages 1-6, Gyeongju, Republic of Korea. Association for Computational Linguistics.\n",
    "\n",
    "8. Cohan, Arman, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020. \"SPECTER: Document-level representation learning using citation-informed transformers.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270-2282, Online. Association for Computational Linguistics.\n",
    "\n",
    "9. DeYoung, Jay, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. \"MS²: Multi-document summarization of medical studies.\" In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7494-7513, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n",
    "\n",
    "10. Erkan, Günes, and Dragomir R Radev. 2004. \"LexRank: Graph-based lexical centrality as salience in text summarization.\" Journal of artificial intelligence research, 22: 457-479.\n",
    "\n",
    "11. Fabbri, Alexander, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. \"Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model.\" In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074-1084, Florence, Italy. Association for Computational Linguistics.\n",
    "\n",
    "12. Hallgren, Kevin A. 2012. \"Computing inter-rater reliability for observational data: an overview and tutorial.\" Tutorials in quantitative methods for psychology, 8(1): 23-34.\n",
    "\n",
    "13. Hossain, MD Zakir, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. 2019. \"A comprehensive survey of deep learning for image captioning.\" ACM Computing Surveys, 51(6): 1-36.\n",
    "\n",
    "14. Izacard, Gautier, and Edouard Grave. 2021. \"Leveraging passage retrieval with generative models for open domain question answering.\" In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics.\n",
    "\n",
    "15. Jaidka, Kokil, Christopher Khoo, and Jin-Cheon Na. 2013a. \"Deconstructing human literature reviews–a framework for multi-document summarization.\" In Proceedings of the 14th European Workshop on Natural Language Generation, pages 125-135, Sofia, Bulgaria. Association for Computational Linguistics.\n",
    "\n",
    "16. Jaidka, Kokil, Christopher SG Khoo, and Jin-Cheon Na. 2013b. \"Literature review writing: how information is selected and transformed.\" In Aslib Proceedings, volume 65, pages 303-325.\n",
    "\n",
    "17. Jaidka, Kokil, Christopher SG Khoo, and Jin-Cheon Na. 2019. \"Characterizing human summarization strategies for text reuse and transformation in literature review writing.\" Scientometrics, 121(3): 1563-1582.\n",
    "\n",
    "18. Ji, Ziwei, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022. \"Survey of hallucination in natural language generation.\" ACM Computing Surveys. Just Accepted.\n",
    "\n",
    "19. Jiao, Licheng, and Jin Zhao. 2019. \"A survey on the new generation of deep learning in image processing.\" IEEE Access, 7: 172231-172263.\n",
    "\n",
    "20. Khan, Khalid S, Regina Kunz, Jos Kleijnen, and Gerd Antes. 2003. \"Five steps to conducting a systematic review.\" Journal of the royal society of medicine, 96(3): 118-121.\n",
    "\n",
    "21. Laga, Hamid. 2019. \"A survey on deep learning architectures for image-based depth reconstruction.\" arXiv preprint arXiv:1906.06113.\n",
    "\n",
    "22. Laskar, Md Tahmid Rahman, Enamul Hoque, and Jimmy Xiangji Huang. 2022. \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization.\" Computational Linguistics, 48(2): 279-320.\n",
    "\n",
    "23. LeCun, Yann, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. \"Backpropagation applied to handwritten zip code recognition.\" Neural computation, 1(4): 541-551.\n",
    "\n",
    "24. Lewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. \"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.\n",
    "\n",
    "25. Lin, Chin-Yew. 2004. \"ROUGE: A package for automatic evaluation of summaries.\" In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.\n",
    "\n",
    "26. Liu, Ruijun, Yuqian Shi, Changjiang Ji, and Ming Jia. 2019. \"A survey of sentiment analysis based on transfer learning.\" IEEE Access, 7: 85401-85412.\n",
    "\n",
    "27. Lo, Kyle, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. \"S2ORC: The semantic scholar open research corpus.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.\n",
    "\n",
    "28. Lu, Yao, Yue Dong, and Laurent Charlin. 2020. \"Multi-XScience: A large-scale dataset for extreme multi-document summarization of scientific articles.\" In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8068-8074, Online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "1. Agarwal, Nitin, Ravi Shankar Reddy, Kiran Gvr, and Carolyn Penstein Rosé. 2011. \"Towards multi-document summarization of scientific articles: making interesting comparisons with SciSumm.\" In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 8-15, Portland, Oregon. Association for Computational Linguistics.\n",
    "\n",
    "2. Beltagy, Iz, Kyle Lo, and Arman Cohan. 2019. \"SciBERT: A pretrained language model for scientific text.\" In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615-3620, Hong Kong, China. Association for Computational Linguistics.\n",
    "\n",
    "3. Beltagy, Iz, Matthew E Peters, and Arman Cohan. 2020. \"Longformer: The long-document transformer.\" arXiv preprint arXiv:2004.05150.\n",
    "\n",
    "4. Bornmann, Lutz, and Rüdiger Mutz. 2015. \"Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references.\" Journal of the Association for Information Science and Technology, 66(11): 2215-2222.\n",
    "\n",
    "5. Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. \"Language models are few-shot learners.\" In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901.\n",
    "\n",
    "6. Cohan, Arman, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. \"A discourse-aware attention model for abstractive summarization of long documents.\" In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computational Linguistics.\n",
    "\n",
    "7. Cohan, Arman, Guy Feigenblat, Dayne Freitag, Tirthankar Ghosal, Drahomira Herrmannova, Petr Knoth, Kyle Lo, Philipp Mayr, Michal Shmueli-Scheuer, Anita de Waard, and Lucy Lu Wang. 2022. \"Overview of the third workshop on scholarly document processing.\" In Proceedings of the Third Workshop on Scholarly Document Processing, pages 1-6, Gyeongju, Republic of Korea. Association for Computational Linguistics.\n",
    "\n",
    "8. Cohan, Arman, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020. \"SPECTER: Document-level representation learning using citation-informed transformers.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270-2282, Online. Association for Computational Linguistics.\n",
    "\n",
    "9. DeYoung, Jay, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. \"MS²: Multi-document summarization of medical studies.\" In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7494-7513, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n",
    "\n",
    "10. Erkan, Günes, and Dragomir R Radev. 2004. \"LexRank: Graph-based lexical centrality as salience in text summarization.\" Journal of artificial intelligence research, 22: 457-479.\n",
    "\n",
    "11. Fabbri, Alexander, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. \"Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model.\" In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074-1084, Florence, Italy. Association for Computational Linguistics.\n",
    "\n",
    "12. Hallgren, Kevin A. 2012. \"Computing inter-rater reliability for observational data: an overview and tutorial.\" Tutorials in quantitative methods for psychology, 8(1): 23-34.\n",
    "\n",
    "13. Hossain, MD Zakir, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. 2019. \"A comprehensive survey of deep learning for image captioning.\" ACM Computing Surveys, 51(6): 1-36.\n",
    "\n",
    "14. Izacard, Gautier, and Edouard Grave. 2021. \"Leveraging passage retrieval with generative models for open domain question answering.\" In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics.\n",
    "\n",
    "15. Jaidka, Kokil, Christopher Khoo, and Jin-Cheon Na. 2013a. \"Deconstructing human literature reviews–a framework for multi-document summarization.\" In Proceedings of the 14th European Workshop on Natural Language Generation, pages 125-135, Sofia, Bulgaria. Association for Computational Linguistics.\n",
    "\n",
    "16. Jaidka, Kokil, Christopher SG Khoo, and Jin-Cheon Na. 2013b. \"Literature review writing: how information is selected and transformed.\" In Aslib Proceedings, volume 65, pages 303-325.\n",
    "\n",
    "17. Jaidka, Kokil, Christopher SG Khoo, and Jin-Cheon Na. 2019. \"Characterizing human summarization strategies for text reuse and transformation in literature review writing.\" Scientometrics, 121(3): 1563-1582.\n",
    "\n",
    "18. Ji, Ziwei, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022. \"Survey of hallucination in natural language generation.\" ACM Computing Surveys. Just Accepted.\n",
    "\n",
    "19. Jiao, Licheng, and Jin Zhao. 2019. \"A survey on the new generation of deep learning in image processing.\" IEEE Access, 7: 172231-172263.\n",
    "\n",
    "20. Khan, Khalid S, Regina Kunz, Jos Kleijnen, and Gerd Antes. 2003. \"Five steps to conducting a systematic review.\" Journal of the royal society of medicine, 96(3): 118-121.\n",
    "\n",
    "21. Laga, Hamid. 2019. \"A survey on deep learning architectures for image-based depth reconstruction.\" arXiv preprint arXiv:1906.06113.\n",
    "\n",
    "22. Laskar, Md Tahmid Rahman, Enamul Hoque, and Jimmy Xiangji Huang. 2022. \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization.\" Computational Linguistics, 48(2): 279-320.\n",
    "\n",
    "23. LeCun, Yann, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. \"Backpropagation applied to handwritten zip code recognition.\" Neural computation, 1(4): 541-551.\n",
    "\n",
    "24. Lewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. \"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.\n",
    "\n",
    "25. Lin, Chin-Yew. 2004. \"ROUGE: A package for automatic evaluation of summaries.\" In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.\n",
    "\n",
    "26. Liu, Ruijun, Yuqian Shi, Changjiang Ji, and Ming Jia. 2019. \"A survey of sentiment analysis based on transfer learning.\" IEEE Access, 7: 85401-85412.\n",
    "\n",
    "27. Lo, Kyle, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. \"S2ORC: The semantic scholar open research corpus.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.\n",
    "\n",
    "28. Lu, Yao, Yue Dong, and Laurent Charlin. 2020. \"Multi-XScience: A large-scale dataset for extreme multi-document summarization of scientific articles.\" In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8068-8074, Online.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# 分割为单独的参考文献条目\n",
    "pattern_split = r'\\n(?=\\d+\\. )'\n",
    "references_list = re.split(pattern_split, text)\n",
    "references_list = [ref for ref in references_list if ref.strip()]  # 移除空条目\n",
    "\n",
    "# 提取信息\n",
    "results = []\n",
    "for i, ref in enumerate(references_list):\n",
    "    # 提取作者和年份\n",
    "    author_year_match = re.match(\n",
    "        r'^\\d+\\.\\s+(.+?)\\.\\s+(\\d{4}[a-z]?)\\.', \n",
    "        ref\n",
    "    )\n",
    "    if author_year_match:\n",
    "        authors = author_year_match.group(1).strip()\n",
    "        year = author_year_match.group(2)\n",
    "    else:\n",
    "        authors = \"未找到作者\"\n",
    "        year = \"未找到年份\"\n",
    "    \n",
    "    # 提取文章标题 (引号中的内容)\n",
    "    title_match = re.search(r'\"([^\"]+)\"', ref)\n",
    "    if title_match:\n",
    "        title = title_match.group(1)\n",
    "    else:\n",
    "        title = \"未找到标题\"\n",
    "    results.append({\n",
    "        \"作者列表\": authors,\n",
    "        \"发表时间\": year,\n",
    "        \"文章名称\": title\n",
    "    })\n",
    "\n",
    "# 转换为JSON\n",
    "results_json = json.dumps(results, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 保存json文件\n",
    "with open('references.json', 'w') as f:\n",
    "    f.write(results_json)\n",
    "\n",
    "with open('references.json', 'r', encoding='utf-8') as f:\n",
    "    loaded_results = json.load(f)\n",
    "assert loaded_results == results\n",
    "assert len(results) == 28\n",
    "assert results[2] == {'作者列表': 'Beltagy, Iz, Matthew E Peters, and Arman Cohan', '发表时间': '2020', '文章名称': 'Longformer: The long-document transformer.'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
