{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五次课后练习 之二（选做）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本次作业中，如果你在配置环境中遇到了问题，可以参考助教的解决方案：\n",
    "\n",
    "首先我们在将这个文件放在一个文件夹中（可以和这门课程的其他作业放在一起，但是别放在桌面这种一大堆其他文件的文件夹里）。\n",
    "\n",
    "随后运行：\n",
    "```\n",
    "python3 -m venv myenv\n",
    "source myenv/bin/activate\n",
    "pip install openai\n",
    "pip install requests\n",
    "pip install socksio\n",
    "```\n",
    "\n",
    "然后在 notebook 中切换 ipykernel 为 myenv（如果你采用的是 VSCode 的话，可以看看右上角的 ipykernel，点一下就可以切换了），就可以运行了。\n",
    "\n",
    "如果你发现还存在包缺失的情况，注意采用 Restart，不然环境可能同步不过来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**负责助教：吴迪**\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">请将作业文件命名为 第五次课后练习-之二+姓名+学号.ipynb, 例如 第五次课后练习-之二+张三+1000000000.ipynb</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  请认真阅读代码，理解学习代码的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.1** 读取网页内容，调用大语言模型API进行中文摘要\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这部分代码进行了重构，主要解决了多次尝试连接、异常处理、日志记录等问题\n",
    "# 代码中使用了retrying库，用于实现重试机制\n",
    "# 代码中使用了logging库，用于记录日志信息\n",
    "# 代码中使用了typing库，用于类型提示\n",
    "import re\n",
    "import requests\n",
    "import html\n",
    "import logging\n",
    "from retrying import retry\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 目标URL列表\n",
    "TARGET_URLS = [\n",
    "    \"https://arxiv.org/abs/2410.03761\",\n",
    "    \"https://arxiv.org/abs/2305.15186\"\n",
    "]\n",
    "\n",
    "# API配置\n",
    "API_ENDPOINT = \"https://openrouter.ai/api/v1\"\n",
    "API_KEY = \"<API_KEY>\"  \n",
    "\n",
    "# 请求头配置\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# 模型配置\n",
    "MODEL_CONFIG = {\n",
    "    \"model\": \"deepseek/deepseek-chat:free\",\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "# 正则表达式，用于解析网页摘要（读取html网页对象的content字段内容）\n",
    "pattern = r'<meta\\s+property=\"og:description\"\\s+content=\"(.*?)\"\\s*/>'\n",
    "\n",
    "class SummaryGenerator:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(\n",
    "            base_url=API_ENDPOINT,\n",
    "            api_key=API_KEY,\n",
    "        )\n",
    "\n",
    "    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def generate_summary(self, text: str) -> Optional[str]:\n",
    "        \"\"\"生成文本摘要（带重试机制）\"\"\"\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                **MODEL_CONFIG,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"请用中文撰写用户文本的300字摘要，保持专业学术风格\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": text\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成摘要失败: {e}\")\n",
    "            return None\n",
    "\n",
    "class ArxivParser:\n",
    "    @staticmethod\n",
    "    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def fetch_content(url: str) -> Optional[str]:\n",
    "        \"\"\"获取网页内容（带重试机制）\"\"\"\n",
    "        try:\n",
    "            # 发送HTTP GET请求获取网页内容\n",
    "            response = requests.get(url, headers=HEADERS, timeout=15) # 设置超时时间\n",
    "            response.raise_for_status() # 检查请求是否成功\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"获取内容失败[{url}]: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_content(html_content: str) -> Optional[str]:\n",
    "        \"\"\"解析网页内容\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            meta_tag = soup.find('meta', {'property': 'og:description'})\n",
    "            if meta_tag and (content := meta_tag.get('content')):\n",
    "                return html.unescape(content)\n",
    "            logger.warning(\"未找到摘要内容\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"解析内容失败: {e}\")\n",
    "            return None\n",
    "\n",
    "def process_url(url: str, generator: SummaryGenerator) -> Optional[Dict]:\n",
    "    \"\"\"处理单个URL的完整流程\"\"\"\n",
    "    logger.info(f\"正在处理: {url}\")\n",
    "    \n",
    "    # 获取网页内容\n",
    "    if (html_content := ArxivParser.fetch_content(url)) is None:\n",
    "        return None\n",
    "    \n",
    "    # 解析摘要\n",
    "    if (summary := ArxivParser.parse_content(html_content)) is None:\n",
    "        return None\n",
    "    \n",
    "    # 生成中文摘要\n",
    "    if (zh_summary := generator.generate_summary(summary)) is None:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"original_summary\": summary,\n",
    "        \"chinese_summary\": zh_summary\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    generator = SummaryGenerator()\n",
    "    all_summaries = []\n",
    "    \n",
    "    for url in TARGET_URLS:\n",
    "        if result := process_url(url, generator):\n",
    "            logger.info(f\"成功处理: {url}\")\n",
    "            all_summaries.append(result)\n",
    "            # 打印当前结果\n",
    "            print(f\"\\n论文地址: {result['url']}\")\n",
    "            print(\"原始摘要:\", result['original_summary'])\n",
    "            print(\"中文摘要:\", result['chinese_summary'])\n",
    "    \n",
    "    # 生成综合摘要（限制输入长度）\n",
    "    combined = \" \".join([s['original_summary'] for s in all_summaries])\n",
    "    if len(combined) > 3000:\n",
    "        combined = combined[:3000] + \"...[截断]\"\n",
    "    \n",
    "    if final_summary := generator.generate_summary(combined):\n",
    "        print(\"\\n综合摘要:\", final_summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 仿照上面的框架，实现一个自己版本的多文档（或多轮对话）的分析功能。可以用更复杂的爬虫获得需要的信息。也可以直接从本地读取数据文件实现有实际意义的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成有新意有创意工作的同学，或在作业过程中觉得有心得或者自己拓展学习到有价值内容的，可以在文件名最后加一个#号。例如第五次课后练习+张三+1000000000+#.ipynb\n",
    "只是完成学习的同学，没有尝试改进探索工作的可以不提交这个作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个代码实现了进行本地学术论文分析\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pdfplumber\n",
    "import fitz  \n",
    "from retrying import retry\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Optional\n",
    "from collections import Counter\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 支持的文件类型\n",
    "SUPPORTED_EXT = ['.pdf', '.txt']\n",
    "\n",
    "# 模型配置\n",
    "MODEL_CONFIG = {\n",
    "    \"model\": \"deepseek/deepseek-chat:free\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.3,\n",
    "}\n",
    "\n",
    "class PDFAnalyzer:\n",
    "    @staticmethod\n",
    "    def extract_metadata(file_path: str) -> Dict:\n",
    "        \"\"\"提取PDF元数据\"\"\"\n",
    "        try:\n",
    "            with fitz.open(file_path) as doc:\n",
    "                meta = doc.metadata\n",
    "                return {\n",
    "                    \"title\": meta.get(\"title\", \"\"),\n",
    "                    \"author\": meta.get(\"author\", \"\"),\n",
    "                    \"keywords\": meta.get(\"keywords\", \"\"),\n",
    "                    \"pages\": len(doc)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"元数据提取失败: {e}\")\n",
    "            return {}\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_text(file_path: str) -> Optional[str]:\n",
    "        \"\"\"专业PDF文本提取（优先获取正文内容）\"\"\"\n",
    "        try:\n",
    "            full_text = []\n",
    "            \n",
    "            # 策略一：使用pdfplumber提取结构化文本\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    # 优先提取正文（排除页眉页脚）\n",
    "                    y0 = page.height * 0.1  # 排除顶部10%\n",
    "                    y1 = page.height * 0.9  # 排除底部10%\n",
    "                    cropped = page.crop((0, y0, page.width, y1))\n",
    "                    text = cropped.extract_text()\n",
    "                    if text:\n",
    "                        # 清理换行符\n",
    "                        text = re.sub(r'-\\n', '', text)\n",
    "                        text = re.sub(r'\\n', ' ', text)\n",
    "                        full_text.append(text)\n",
    "            \n",
    "            # 策略二：如果方法一失败，使用PyMuPDF的智能提取\n",
    "            if not full_text:\n",
    "                with fitz.open(file_path) as doc:\n",
    "                    for page in doc:\n",
    "                        text = page.get_text(\"text\")\n",
    "                        full_text.append(text)\n",
    "\n",
    "            return \"\\n\".join(full_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDF解析失败: {e}\")\n",
    "            return None\n",
    "\n",
    "class FileProcessor:\n",
    "    @staticmethod\n",
    "    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def read_content(file_path: str) -> Optional[Dict]:\n",
    "        \"\"\"智能文件读取（自动判断文件类型）\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.warning(f\"文件不存在: {file_path}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            file_type = os.path.splitext(file_path)[1].lower()\n",
    "            \n",
    "            if file_type == '.pdf':\n",
    "                # 提取PDF元数据\n",
    "                meta = PDFAnalyzer.extract_metadata(file_path)\n",
    "                # 提取正文内容\n",
    "                if (text := PDFAnalyzer.extract_text(file_path)):\n",
    "                    return {\"content\": text, \"meta\": meta}\n",
    "            \n",
    "            elif file_type == '.txt':\n",
    "                # 文本文件读取（自动检测编码）\n",
    "                for encoding in ['utf-8', 'gbk', 'latin-1']:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding=encoding) as f:\n",
    "                            return {\"content\": f.read(), \"meta\": {}}\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            \n",
    "            logger.error(f\"不支持的格式: {file_type}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"文件读取失败[{file_path}]: {e}\")\n",
    "            return None\n",
    "\n",
    "class ResearchPaperAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=\"<API_KEY>\",\n",
    "        )\n",
    "        self.keyword_counter = Counter()\n",
    "        self.domain_counter = Counter()\n",
    "\n",
    "    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def analyze_paper(self, content: str) -> Optional[Dict]:\n",
    "        \"\"\"学术论文深度分析\"\"\"\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                **MODEL_CONFIG,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"您是一名学术研究助理，请分析论文内容并返回：\\n\"\n",
    "                                   \"1. 核心研究问题\\n\"\n",
    "                                   \"2. 5个关键技术术语\\n\"\n",
    "                                   \"3. 研究方法（50字）\\n\"\n",
    "                                   \"4. 主要结论（50字）\\n\"\n",
    "                                   \"5. 所属学科领域（最多3个）\\n\"\n",
    "                                   \"格式：JSON，包含problem, keywords, methodology, conclusion, fields\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content[:6000]  # 限制输入长度\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            return self._parse_response(completion.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析失败: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_response(self, response: str) -> Optional[Dict]:\n",
    "        \"\"\"解析模型响应\"\"\"\n",
    "        try:\n",
    "            # 提取JSON部分\n",
    "            json_str = re.search(r'\\{.*\\}', response, re.DOTALL).group()\n",
    "            result = eval(json_str)\n",
    "            \n",
    "            # 数据校验\n",
    "            required_keys = ['problem', 'keywords', 'methodology', 'conclusion', 'fields']\n",
    "            if all(key in result for key in required_keys):\n",
    "                # 统计关键词和领域\n",
    "                self.keyword_counter.update(result['keywords'])\n",
    "                self.domain_counter.update(result['fields'])\n",
    "                return result\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"响应解析失败: {e}\")\n",
    "            return None\n",
    "\n",
    "def generate_visual_report(analyzer: ResearchPaperAnalyzer) -> str:\n",
    "    \"\"\"生成可视化报告\"\"\"\n",
    "    report = [\"\\n学术论文分析报告\", \"=\"*40]\n",
    "    \n",
    "    # 关键词云\n",
    "    top_keywords = analyzer.keyword_counter.most_common(10)\n",
    "    report.append(\"\\n🔍 高频关键词TOP10：\")\n",
    "    report.extend([f\"- {word[0]} ({word[1]}次)\" for word in top_keywords])\n",
    "    \n",
    "    # 领域分布\n",
    "    report.append(\"\\n📚 学科领域分布：\")\n",
    "    for domain, count in analyzer.domain_counter.most_common():\n",
    "        report.append(f\"- {domain}: {count}篇\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "def main():\n",
    "    # 示例文件路径（需修改为实际路径）\n",
    "    papers = [\"data/report.pdf\"]\n",
    "    \n",
    "    analyzer = ResearchPaperAnalyzer()\n",
    "    \n",
    "    for paper_path in papers:\n",
    "        logger.info(f\"正在分析: {paper_path}\")\n",
    "        \n",
    "        # 读取文件\n",
    "        if not (data := FileProcessor.read_content(paper_path)):\n",
    "            continue\n",
    "            \n",
    "        # 执行分析\n",
    "        if not (result := analyzer.analyze_paper(data['content'])):\n",
    "            continue\n",
    "            \n",
    "        # 打印结果\n",
    "        print(f\"\\n📄 论文: {os.path.basename(paper_path)}\")\n",
    "        if data['meta'].get('title'):\n",
    "            print(f\"标题: {data['meta']['title']}\")\n",
    "        print(f\"领域: {', '.join(result['fields'])}\")\n",
    "        print(f\"核心问题: {result['problem']}\")\n",
    "        print(f\"研究方法: {result['methodology']}\")\n",
    "        print(f\"主要结论: {result['conclusion']}\")\n",
    "        print(f\"关键词: {', '.join(result['keywords'])}\")\n",
    "    \n",
    "    # 生成综合报告\n",
    "    print(generate_visual_report(analyzer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
