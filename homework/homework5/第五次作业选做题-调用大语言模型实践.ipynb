{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬äº”æ¬¡è¯¾åç»ƒä¹  ä¹‹äºŒï¼ˆé€‰åšï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æœ¬æ¬¡ä½œä¸šä¸­ï¼Œå¦‚æœä½ åœ¨é…ç½®ç¯å¢ƒä¸­é‡åˆ°äº†é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒåŠ©æ•™çš„è§£å†³æ–¹æ¡ˆï¼š\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬åœ¨å°†è¿™ä¸ªæ–‡ä»¶æ”¾åœ¨ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼ˆå¯ä»¥å’Œè¿™é—¨è¯¾ç¨‹çš„å…¶ä»–ä½œä¸šæ”¾åœ¨ä¸€èµ·ï¼Œä½†æ˜¯åˆ«æ”¾åœ¨æ¡Œé¢è¿™ç§ä¸€å¤§å †å…¶ä»–æ–‡ä»¶çš„æ–‡ä»¶å¤¹é‡Œï¼‰ã€‚\n",
    "\n",
    "éšåè¿è¡Œï¼š\n",
    "```\n",
    "python3 -m venv myenv\n",
    "source myenv/bin/activate\n",
    "pip install openai\n",
    "pip install requests\n",
    "pip install socksio\n",
    "```\n",
    "\n",
    "ç„¶ååœ¨ notebook ä¸­åˆ‡æ¢ ipykernel ä¸º myenvï¼ˆå¦‚æœä½ é‡‡ç”¨çš„æ˜¯ VSCode çš„è¯ï¼Œå¯ä»¥çœ‹çœ‹å³ä¸Šè§’çš„ ipykernelï¼Œç‚¹ä¸€ä¸‹å°±å¯ä»¥åˆ‡æ¢äº†ï¼‰ï¼Œå°±å¯ä»¥è¿è¡Œäº†ã€‚\n",
    "\n",
    "å¦‚æœä½ å‘ç°è¿˜å­˜åœ¨åŒ…ç¼ºå¤±çš„æƒ…å†µï¼Œæ³¨æ„é‡‡ç”¨ Restartï¼Œä¸ç„¶ç¯å¢ƒå¯èƒ½åŒæ­¥ä¸è¿‡æ¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è´Ÿè´£åŠ©æ•™ï¼šå´è¿ª**\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">è¯·å°†ä½œä¸šæ–‡ä»¶å‘½åä¸º ç¬¬äº”æ¬¡è¯¾åç»ƒä¹ -ä¹‹äºŒ+å§“å+å­¦å·.ipynb, ä¾‹å¦‚ ç¬¬äº”æ¬¡è¯¾åç»ƒä¹ -ä¹‹äºŒ+å¼ ä¸‰+1000000000.ipynb</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  è¯·è®¤çœŸé˜…è¯»ä»£ç ï¼Œç†è§£å­¦ä¹ ä»£ç çš„åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.1** è¯»å–ç½‘é¡µå†…å®¹ï¼Œè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹APIè¿›è¡Œä¸­æ–‡æ‘˜è¦\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿™éƒ¨åˆ†ä»£ç è¿›è¡Œäº†é‡æ„ï¼Œä¸»è¦è§£å†³äº†å¤šæ¬¡å°è¯•è¿æ¥ã€å¼‚å¸¸å¤„ç†ã€æ—¥å¿—è®°å½•ç­‰é—®é¢˜\n",
    "# ä»£ç ä¸­ä½¿ç”¨äº†retryingåº“ï¼Œç”¨äºå®ç°é‡è¯•æœºåˆ¶\n",
    "# ä»£ç ä¸­ä½¿ç”¨äº†loggingåº“ï¼Œç”¨äºè®°å½•æ—¥å¿—ä¿¡æ¯\n",
    "# ä»£ç ä¸­ä½¿ç”¨äº†typingåº“ï¼Œç”¨äºç±»å‹æç¤º\n",
    "import re\n",
    "import requests\n",
    "import html\n",
    "import logging\n",
    "from retrying import retry\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# é…ç½®æ—¥å¿—\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ç›®æ ‡URLåˆ—è¡¨\n",
    "TARGET_URLS = [\n",
    "    \"https://arxiv.org/abs/2410.03761\",\n",
    "    \"https://arxiv.org/abs/2305.15186\"\n",
    "]\n",
    "\n",
    "# APIé…ç½®\n",
    "API_ENDPOINT = \"https://openrouter.ai/api/v1\"\n",
    "API_KEY = \"<API_KEY>\"  \n",
    "\n",
    "# è¯·æ±‚å¤´é…ç½®\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# æ¨¡å‹é…ç½®\n",
    "MODEL_CONFIG = {\n",
    "    \"model\": \"deepseek/deepseek-chat:free\",\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "# æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºè§£æç½‘é¡µæ‘˜è¦ï¼ˆè¯»å–htmlç½‘é¡µå¯¹è±¡çš„contentå­—æ®µå†…å®¹ï¼‰\n",
    "pattern = r'<meta\\s+property=\"og:description\"\\s+content=\"(.*?)\"\\s*/>'\n",
    "\n",
    "class SummaryGenerator:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(\n",
    "            base_url=API_ENDPOINT,\n",
    "            api_key=API_KEY,\n",
    "        )\n",
    "\n",
    "    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def generate_summary(self, text: str) -> Optional[str]:\n",
    "        \"\"\"ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰\"\"\"\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                **MODEL_CONFIG,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"è¯·ç”¨ä¸­æ–‡æ’°å†™ç”¨æˆ·æ–‡æœ¬çš„300å­—æ‘˜è¦ï¼Œä¿æŒä¸“ä¸šå­¦æœ¯é£æ ¼\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": text\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "class ArxivParser:\n",
    "    @staticmethod\n",
    "    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def fetch_content(url: str) -> Optional[str]:\n",
    "        \"\"\"è·å–ç½‘é¡µå†…å®¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰\"\"\"\n",
    "        try:\n",
    "            # å‘é€HTTP GETè¯·æ±‚è·å–ç½‘é¡µå†…å®¹\n",
    "            response = requests.get(url, headers=HEADERS, timeout=15) # è®¾ç½®è¶…æ—¶æ—¶é—´\n",
    "            response.raise_for_status() # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"è·å–å†…å®¹å¤±è´¥[{url}]: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_content(html_content: str) -> Optional[str]:\n",
    "        \"\"\"è§£æç½‘é¡µå†…å®¹\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            meta_tag = soup.find('meta', {'property': 'og:description'})\n",
    "            if meta_tag and (content := meta_tag.get('content')):\n",
    "                return html.unescape(content)\n",
    "            logger.warning(\"æœªæ‰¾åˆ°æ‘˜è¦å†…å®¹\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£æå†…å®¹å¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "def process_url(url: str, generator: SummaryGenerator) -> Optional[Dict]:\n",
    "    \"\"\"å¤„ç†å•ä¸ªURLçš„å®Œæ•´æµç¨‹\"\"\"\n",
    "    logger.info(f\"æ­£åœ¨å¤„ç†: {url}\")\n",
    "    \n",
    "    # è·å–ç½‘é¡µå†…å®¹\n",
    "    if (html_content := ArxivParser.fetch_content(url)) is None:\n",
    "        return None\n",
    "    \n",
    "    # è§£ææ‘˜è¦\n",
    "    if (summary := ArxivParser.parse_content(html_content)) is None:\n",
    "        return None\n",
    "    \n",
    "    # ç”Ÿæˆä¸­æ–‡æ‘˜è¦\n",
    "    if (zh_summary := generator.generate_summary(summary)) is None:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"original_summary\": summary,\n",
    "        \"chinese_summary\": zh_summary\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    generator = SummaryGenerator()\n",
    "    all_summaries = []\n",
    "    \n",
    "    for url in TARGET_URLS:\n",
    "        if result := process_url(url, generator):\n",
    "            logger.info(f\"æˆåŠŸå¤„ç†: {url}\")\n",
    "            all_summaries.append(result)\n",
    "            # æ‰“å°å½“å‰ç»“æœ\n",
    "            print(f\"\\nè®ºæ–‡åœ°å€: {result['url']}\")\n",
    "            print(\"åŸå§‹æ‘˜è¦:\", result['original_summary'])\n",
    "            print(\"ä¸­æ–‡æ‘˜è¦:\", result['chinese_summary'])\n",
    "    \n",
    "    # ç”Ÿæˆç»¼åˆæ‘˜è¦ï¼ˆé™åˆ¶è¾“å…¥é•¿åº¦ï¼‰\n",
    "    combined = \" \".join([s['original_summary'] for s in all_summaries])\n",
    "    if len(combined) > 3000:\n",
    "        combined = combined[:3000] + \"...[æˆªæ–­]\"\n",
    "    \n",
    "    if final_summary := generator.generate_summary(combined):\n",
    "        print(\"\\nç»¼åˆæ‘˜è¦:\", final_summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ä»¿ç…§ä¸Šé¢çš„æ¡†æ¶ï¼Œå®ç°ä¸€ä¸ªè‡ªå·±ç‰ˆæœ¬çš„å¤šæ–‡æ¡£ï¼ˆæˆ–å¤šè½®å¯¹è¯ï¼‰çš„åˆ†æåŠŸèƒ½ã€‚å¯ä»¥ç”¨æ›´å¤æ‚çš„çˆ¬è™«è·å¾—éœ€è¦çš„ä¿¡æ¯ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ä»æœ¬åœ°è¯»å–æ•°æ®æ–‡ä»¶å®ç°æœ‰å®é™…æ„ä¹‰çš„åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®Œæˆæœ‰æ–°æ„æœ‰åˆ›æ„å·¥ä½œçš„åŒå­¦ï¼Œæˆ–åœ¨ä½œä¸šè¿‡ç¨‹ä¸­è§‰å¾—æœ‰å¿ƒå¾—æˆ–è€…è‡ªå·±æ‹“å±•å­¦ä¹ åˆ°æœ‰ä»·å€¼å†…å®¹çš„ï¼Œå¯ä»¥åœ¨æ–‡ä»¶åæœ€ååŠ ä¸€ä¸ª#å·ã€‚ä¾‹å¦‚ç¬¬äº”æ¬¡è¯¾åç»ƒä¹ +å¼ ä¸‰+1000000000+#.ipynb\n",
    "åªæ˜¯å®Œæˆå­¦ä¹ çš„åŒå­¦ï¼Œæ²¡æœ‰å°è¯•æ”¹è¿›æ¢ç´¢å·¥ä½œçš„å¯ä»¥ä¸æäº¤è¿™ä¸ªä½œä¸š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿™ä¸ªä»£ç å®ç°äº†è¿›è¡Œæœ¬åœ°å­¦æœ¯è®ºæ–‡åˆ†æ\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pdfplumber\n",
    "import fitz  \n",
    "from retrying import retry\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Optional\n",
    "from collections import Counter\n",
    "\n",
    "# é…ç½®æ—¥å¿—\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# æ”¯æŒçš„æ–‡ä»¶ç±»å‹\n",
    "SUPPORTED_EXT = ['.pdf', '.txt']\n",
    "\n",
    "# æ¨¡å‹é…ç½®\n",
    "MODEL_CONFIG = {\n",
    "    \"model\": \"deepseek/deepseek-chat:free\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.3,\n",
    "}\n",
    "\n",
    "class PDFAnalyzer:\n",
    "    @staticmethod\n",
    "    def extract_metadata(file_path: str) -> Dict:\n",
    "        \"\"\"æå–PDFå…ƒæ•°æ®\"\"\"\n",
    "        try:\n",
    "            with fitz.open(file_path) as doc:\n",
    "                meta = doc.metadata\n",
    "                return {\n",
    "                    \"title\": meta.get(\"title\", \"\"),\n",
    "                    \"author\": meta.get(\"author\", \"\"),\n",
    "                    \"keywords\": meta.get(\"keywords\", \"\"),\n",
    "                    \"pages\": len(doc)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å…ƒæ•°æ®æå–å¤±è´¥: {e}\")\n",
    "            return {}\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_text(file_path: str) -> Optional[str]:\n",
    "        \"\"\"ä¸“ä¸šPDFæ–‡æœ¬æå–ï¼ˆä¼˜å…ˆè·å–æ­£æ–‡å†…å®¹ï¼‰\"\"\"\n",
    "        try:\n",
    "            full_text = []\n",
    "            \n",
    "            # ç­–ç•¥ä¸€ï¼šä½¿ç”¨pdfplumberæå–ç»“æ„åŒ–æ–‡æœ¬\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    # ä¼˜å…ˆæå–æ­£æ–‡ï¼ˆæ’é™¤é¡µçœ‰é¡µè„šï¼‰\n",
    "                    y0 = page.height * 0.1  # æ’é™¤é¡¶éƒ¨10%\n",
    "                    y1 = page.height * 0.9  # æ’é™¤åº•éƒ¨10%\n",
    "                    cropped = page.crop((0, y0, page.width, y1))\n",
    "                    text = cropped.extract_text()\n",
    "                    if text:\n",
    "                        # æ¸…ç†æ¢è¡Œç¬¦\n",
    "                        text = re.sub(r'-\\n', '', text)\n",
    "                        text = re.sub(r'\\n', ' ', text)\n",
    "                        full_text.append(text)\n",
    "            \n",
    "            # ç­–ç•¥äºŒï¼šå¦‚æœæ–¹æ³•ä¸€å¤±è´¥ï¼Œä½¿ç”¨PyMuPDFçš„æ™ºèƒ½æå–\n",
    "            if not full_text:\n",
    "                with fitz.open(file_path) as doc:\n",
    "                    for page in doc:\n",
    "                        text = page.get_text(\"text\")\n",
    "                        full_text.append(text)\n",
    "\n",
    "            return \"\\n\".join(full_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDFè§£æå¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "class FileProcessor:\n",
    "    @staticmethod\n",
    "    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def read_content(file_path: str) -> Optional[Dict]:\n",
    "        \"\"\"æ™ºèƒ½æ–‡ä»¶è¯»å–ï¼ˆè‡ªåŠ¨åˆ¤æ–­æ–‡ä»¶ç±»å‹ï¼‰\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.warning(f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            file_type = os.path.splitext(file_path)[1].lower()\n",
    "            \n",
    "            if file_type == '.pdf':\n",
    "                # æå–PDFå…ƒæ•°æ®\n",
    "                meta = PDFAnalyzer.extract_metadata(file_path)\n",
    "                # æå–æ­£æ–‡å†…å®¹\n",
    "                if (text := PDFAnalyzer.extract_text(file_path)):\n",
    "                    return {\"content\": text, \"meta\": meta}\n",
    "            \n",
    "            elif file_type == '.txt':\n",
    "                # æ–‡æœ¬æ–‡ä»¶è¯»å–ï¼ˆè‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼‰\n",
    "                for encoding in ['utf-8', 'gbk', 'latin-1']:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding=encoding) as f:\n",
    "                            return {\"content\": f.read(), \"meta\": {}}\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            \n",
    "            logger.error(f\"ä¸æ”¯æŒçš„æ ¼å¼: {file_type}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"æ–‡ä»¶è¯»å–å¤±è´¥[{file_path}]: {e}\")\n",
    "            return None\n",
    "\n",
    "class ResearchPaperAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=\"<API_KEY>\",\n",
    "        )\n",
    "        self.keyword_counter = Counter()\n",
    "        self.domain_counter = Counter()\n",
    "\n",
    "    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def analyze_paper(self, content: str) -> Optional[Dict]:\n",
    "        \"\"\"å­¦æœ¯è®ºæ–‡æ·±åº¦åˆ†æ\"\"\"\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                **MODEL_CONFIG,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"æ‚¨æ˜¯ä¸€åå­¦æœ¯ç ”ç©¶åŠ©ç†ï¼Œè¯·åˆ†æè®ºæ–‡å†…å®¹å¹¶è¿”å›ï¼š\\n\"\n",
    "                                   \"1. æ ¸å¿ƒç ”ç©¶é—®é¢˜\\n\"\n",
    "                                   \"2. 5ä¸ªå…³é”®æŠ€æœ¯æœ¯è¯­\\n\"\n",
    "                                   \"3. ç ”ç©¶æ–¹æ³•ï¼ˆ50å­—ï¼‰\\n\"\n",
    "                                   \"4. ä¸»è¦ç»“è®ºï¼ˆ50å­—ï¼‰\\n\"\n",
    "                                   \"5. æ‰€å±å­¦ç§‘é¢†åŸŸï¼ˆæœ€å¤š3ä¸ªï¼‰\\n\"\n",
    "                                   \"æ ¼å¼ï¼šJSONï¼ŒåŒ…å«problem, keywords, methodology, conclusion, fields\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content[:6000]  # é™åˆ¶è¾“å…¥é•¿åº¦\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            return self._parse_response(completion.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"åˆ†æå¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_response(self, response: str) -> Optional[Dict]:\n",
    "        \"\"\"è§£ææ¨¡å‹å“åº”\"\"\"\n",
    "        try:\n",
    "            # æå–JSONéƒ¨åˆ†\n",
    "            json_str = re.search(r'\\{.*\\}', response, re.DOTALL).group()\n",
    "            result = eval(json_str)\n",
    "            \n",
    "            # æ•°æ®æ ¡éªŒ\n",
    "            required_keys = ['problem', 'keywords', 'methodology', 'conclusion', 'fields']\n",
    "            if all(key in result for key in required_keys):\n",
    "                # ç»Ÿè®¡å…³é”®è¯å’Œé¢†åŸŸ\n",
    "                self.keyword_counter.update(result['keywords'])\n",
    "                self.domain_counter.update(result['fields'])\n",
    "                return result\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å“åº”è§£æå¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "def generate_visual_report(analyzer: ResearchPaperAnalyzer) -> str:\n",
    "    \"\"\"ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š\"\"\"\n",
    "    report = [\"\\nå­¦æœ¯è®ºæ–‡åˆ†ææŠ¥å‘Š\", \"=\"*40]\n",
    "    \n",
    "    # å…³é”®è¯äº‘\n",
    "    top_keywords = analyzer.keyword_counter.most_common(10)\n",
    "    report.append(\"\\nğŸ” é«˜é¢‘å…³é”®è¯TOP10ï¼š\")\n",
    "    report.extend([f\"- {word[0]} ({word[1]}æ¬¡)\" for word in top_keywords])\n",
    "    \n",
    "    # é¢†åŸŸåˆ†å¸ƒ\n",
    "    report.append(\"\\nğŸ“š å­¦ç§‘é¢†åŸŸåˆ†å¸ƒï¼š\")\n",
    "    for domain, count in analyzer.domain_counter.most_common():\n",
    "        report.append(f\"- {domain}: {count}ç¯‡\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "def main():\n",
    "    # ç¤ºä¾‹æ–‡ä»¶è·¯å¾„ï¼ˆéœ€ä¿®æ”¹ä¸ºå®é™…è·¯å¾„ï¼‰\n",
    "    papers = [\"data/report.pdf\"]\n",
    "    \n",
    "    analyzer = ResearchPaperAnalyzer()\n",
    "    \n",
    "    for paper_path in papers:\n",
    "        logger.info(f\"æ­£åœ¨åˆ†æ: {paper_path}\")\n",
    "        \n",
    "        # è¯»å–æ–‡ä»¶\n",
    "        if not (data := FileProcessor.read_content(paper_path)):\n",
    "            continue\n",
    "            \n",
    "        # æ‰§è¡Œåˆ†æ\n",
    "        if not (result := analyzer.analyze_paper(data['content'])):\n",
    "            continue\n",
    "            \n",
    "        # æ‰“å°ç»“æœ\n",
    "        print(f\"\\nğŸ“„ è®ºæ–‡: {os.path.basename(paper_path)}\")\n",
    "        if data['meta'].get('title'):\n",
    "            print(f\"æ ‡é¢˜: {data['meta']['title']}\")\n",
    "        print(f\"é¢†åŸŸ: {', '.join(result['fields'])}\")\n",
    "        print(f\"æ ¸å¿ƒé—®é¢˜: {result['problem']}\")\n",
    "        print(f\"ç ”ç©¶æ–¹æ³•: {result['methodology']}\")\n",
    "        print(f\"ä¸»è¦ç»“è®º: {result['conclusion']}\")\n",
    "        print(f\"å…³é”®è¯: {', '.join(result['keywords'])}\")\n",
    "    \n",
    "    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š\n",
    "    print(generate_visual_report(analyzer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
