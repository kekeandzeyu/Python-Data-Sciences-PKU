{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第十次课后练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**负责助教：朱轩宇**\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">请将作业文件命名为 第十次课后练习+姓名+学号.ipynb, 例如 第十次课后练习+张三+1000000000.ipynb</span>\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">在作业过程中觉得有心得或者自己拓展学习到有价值内容的，可以在文件名最后加一个#号。例如第十次课后练习+张三+1000000000+#.ipynb</span>\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">本次课同时发布课后练习和选做题，提交时请注意区分提交通道</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第零部分 代码理解\n",
    "\n",
    "请认真阅读代码，理解代码的功能，先写出预想的结果。运行并检验结果是否如预期。如果不如预期，请分析理解其中的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.1** groupby()函数\n",
    "    阅读下面代码，观察运行的结果，解释出现这个结果的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], \n",
    "        'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'], \n",
    "        'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n",
    "        'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}\n",
    "regiment = pd.DataFrame(raw_data, columns = raw_data.keys())\n",
    "regiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1.每个军团的平均测试前成绩是多少\")\n",
    "display(regiment.groupby('regiment')['preTestScore'].mean())\n",
    "\n",
    "print(\"2.呈现每个company的统计数据\")\n",
    "display(regiment.groupby('company')[['preTestScore','postTestScore']].describe())\n",
    "\n",
    "print(\"3.每个company的平均测试前成绩\")\n",
    "display(regiment.groupby('company')['preTestScore'].mean())\n",
    "\n",
    "print(\"4.每个regiment和company的平均测试前成绩\")\n",
    "display(regiment.groupby(['regiment','company'])['preTestScore'].mean())\n",
    "\n",
    "print(\"5.每个company下的regiment有多少条数据\")\n",
    "display(regiment.groupby(['company','regiment']).agg({'name':'count','preTestScore':'sum'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.2** agg() 聚合函数\n",
    "    阅读下面代码，观察运行的结果，解释出现这个结果的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 构建数据\n",
    "df = pd.DataFrame({\n",
    "    '商品名称': ['药品A', '药品B', '药品C', '药品A', '药品B', '药品C'],\n",
    "    '社保卡号': ['123', '123', '456', '456', '789', '789'],\n",
    "    '购药时间': ['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01', '2023-05-01', '2023-06-01'],\n",
    "    '销售数量': [10, 20, 30, 40, 50, 60],\n",
    "    '实收金额': [100, 200, 300, 400, 500, 600]\n",
    "})\n",
    "\n",
    "#查看数据类型\n",
    "print(df)\n",
    "\n",
    "#求不同药品的销售总量，最高销售数量\n",
    "#对某一列进行多种聚合操作\n",
    "print(df.groupby(\"商品名称\")[\"销售数量\"].agg([np.sum, np.max]))\n",
    "print(df.groupby(\"商品名称\")[\"销售数量\"].agg([\"sum\", \"max\"]))\n",
    "\n",
    "#求不同社保卡号的拿药次数和消费总金额\n",
    "print(df.groupby(\"社保卡号\").agg({\"社保卡号\": \"count\", \"实收金额\": \"sum\"}))\n",
    "\n",
    "#求1季度不同药品的总销售金额和总销售数量\n",
    "df[\"购药时间\"]=pd.to_datetime(df[\"购药时间\"])\n",
    "\n",
    "#日期类型的数据如果作为索引，不需要加dt，如果日期类型是一列，需要加dt访问日期对象\n",
    "df[df[\"购药时间\"].dt.quarter==1].groupby(\"商品名称\").agg({\"实收金额\":\"sum\",\"销售数量\":\"sum\"})\n",
    "\n",
    "#求每个月份，不同商品的销售总数量和平均销售额\n",
    "m=df[\"购药时间\"].dt.month\n",
    "print(df.groupby([m,\"商品名称\"]).agg({\"实收金额\": \"mean\", \"销售数量\": \"sum\"}))\n",
    "\n",
    "#统计每月，每种药品的最低和最高销售数量，平均和实收总金额\n",
    "print(df.groupby([m, \"商品名称\"]).agg({\"实收金额\": [\"mean\", \"sum\"], \"销售数量\": [\"max\", \"min\"]}))\n",
    "\n",
    "#统计每月  销售金额的跨度（每月实收金额最大值-每月最小值）\n",
    "#1-7月  分为7个小组，a是每个小组的实收金额的数据\n",
    "print(df.groupby(m)[\"实收金额\"].agg(lambda a: a.max() - a.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.3** 数据可视化\n",
    "    阅读下面代码，观察运行的结果，解释出现这个结果的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 创建一个 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': np.arange(1, 11),\n",
    "    'y': np.random.randn(10)\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 plot() 函数绘制线图\n",
    "df.plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 plot() 函数绘制柱状图\n",
    "df.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 plot() 函数绘制散点图\n",
    "df.plot(kind='scatter', x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': np.random.randn(1000),\n",
    "})\n",
    "\n",
    "# 使用 plot() 函数绘制直方图\n",
    "df['x'].plot(kind='hist', bins=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.4** 中文分词与词频统计\n",
    "    阅读下面代码，观察运行的结果，解释出现这个结果的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4.1 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "# 任务1：基础分词模式（8分）\n",
    "text1 = \"自然语言处理是人工智能的重要方向\"\n",
    "full_mode = \"/\".join(jieba.cut(text1, cut_all=True))   # 补全全模式分词[1,2](@ref)\n",
    "precise_mode = jieba.lcut(text1)                     # 补全精确模式函数[2,3](@ref)\n",
    "search_mode = jieba.lcut_for_search(text1)          # 保留搜索引擎模式分词\n",
    "\n",
    "print(\"全模式:\", full_mode) \n",
    "print(\"精确模式:\", precise_mode)\n",
    "print(\"搜索模式:\", search_mode)\n",
    "\n",
    "# 自定义词典操作\n",
    "# 加载词典文件（格式：词语 词频 词性）\n",
    "custom_dict = \"\"\"人工智能 500 n\n",
    "超导量子芯片 300\n",
    "北京大学 1000 ns\n",
    "jieba 10 x\n",
    "动态新词 200 n\n",
    "\"\"\"\n",
    "with open(\"custom_dict.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_dict)  # 将自定义词典写入文件\n",
    "\n",
    "jieba.load_userdict(\"custom_dict.txt\")\n",
    "text2 = \"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\"\n",
    "seg2 = jieba.lcut(text2)\n",
    "print(\"加载词典前:\", seg2) \n",
    "\n",
    "# 动态添加新词（要求：使\"中国科学院计算所\"不被拆分）\n",
    "jieba.add_word(\"中国科学院计算所\", freq=200)\n",
    "seg2_new = jieba.lcut(text2)\n",
    "print(\"加载词典后:\", seg2_new)\n",
    "\n",
    "#动态调整词典\n",
    "text3 = \"江州市长江大桥设计方案引发争议\"\n",
    "seg3_1 = jieba.lcut(text3)\n",
    "print(\"动态调整前:\", seg3_1)\n",
    "# 调整词频使\"江州/市/长江大桥\"变为\"江州市/长江大桥\"\n",
    "jieba.suggest_freq('江州市', tune=True)  # 调整\"江州市\"\n",
    "seg3 = jieba.lcut(text3)\n",
    "print(\"动态调整后:\", seg3)\n",
    "\n",
    "#关键词抽取\n",
    "text4 = \"\"\"机器学习是人工智能的核心技术，深度学习通过神经网络实现特征学习，\n",
    "          强化学习则注重智能体与环境的交互反馈。\"\"\"\n",
    "# 基于TF-IDF算法提取top3关键词\n",
    "keywords_tfidf = jieba.analyse.extract_tags(text4, topK=3)\n",
    "# 基于TextRank算法提取top2关键词（限定名词和动词）\n",
    "keywords_tr = jieba.analyse.textrank(text4, topK=2, allowPOS=('n','vn','v'))\n",
    "\n",
    "print(\"TF-IDF关键词:\", keywords_tfidf)\n",
    "print(\"TextRank关键词:\", keywords_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4.2 词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据准备\n",
    "text = \"\"\"\n",
    "自然语言处理（NLP）是人工智能领域的重要分支，其核心技术包括词向量表示、文本分类和语义理解。近年来，随着深度学习的发展，Transformer模型如BERT、GPT已在机器翻译、问答系统中取得突破。然而，中文NLP仍面临挑战：例如，中文分词歧义（如“南京市长江大桥”）、未登录词识别（如新网络用语“绝绝子”）等。\n",
    "\n",
    "在社交媒体场景中，用户生成的文本常包含大量噪音。例如：“我今天买了iPhone15，感觉性价比超高！但续航真的太拉胯了……[笑哭]”。这类文本需清洗标点、去除停用词（如“的”“了”）和过滤表情符号。此外，垂直领域（如医疗、法律）的专业术语也需要定制化处理，例如“冠状动脉粥样硬化”应作为整体词汇切分。\n",
    "\n",
    "教育领域的应用同样值得关注。在线学习平台通过分析学生作文，可提取高频错误词汇（如“的地得”混用），并生成个性化学习建议。而政府工作报告中的长句结构（如“坚持稳中求进工作总基调，统筹疫情防控和经济社会发展”）则需结合语义角色标注进行关键信息提取。\n",
    "\"\"\"\n",
    "\n",
    "# 停用词处理\n",
    "stopwords = [\n",
    "    # 基础虚词\n",
    "    \"的\", \"了\", \"是\", \"在\", \"和\", \"与\", \"或\", \"等\", \"而\", \"但\",\n",
    "    # 标点与符号\n",
    "    \"（\", \"）\", \"，\", \"。\", \"！\", \"……\", \"[\", \"]\",\n",
    "    # 高频无意义词\n",
    "    \"例如\", \"例如\", \"例如\", \"这类\", \"需要\", \"例如\",\n",
    "    # 领域相关停用词（根据需求扩展）\n",
    "    \"用户\", \"场景\", \"文本\", \"模型\", \"数据\",  \n",
    "    # 数字与英文（可选过滤项）\n",
    "    \"15\", \"iPhone\", \"BERT\", \"GPT\"\n",
    "]\n",
    "words = jieba.lcut(text)\n",
    "filtered_words = [word for word in words if word not in stopwords and len(word) > 1]\n",
    "\n",
    "# 分类高频词提取\n",
    "# 提取名词（词性为'n'）和动词（词性为'v'）\n",
    "nouns, verbs = [], []\n",
    "for word, flag in jieba.posseg.lcut(text):\n",
    "    if flag.startswith('n') and word not in stopwords:\n",
    "        nouns.append(word)\n",
    "    elif flag.startswith('v') and word not in stopwords:\n",
    "        verbs.append(word)\n",
    "\n",
    "# 统计高频词（取Top5）\n",
    "top_nouns = Counter(nouns).most_common(5)\n",
    "top_verbs = Counter(verbs).most_common(5)\n",
    "\n",
    "print(\"高频名词:\", top_nouns)\n",
    "print(\"高频动词:\", top_verbs)\n",
    "\n",
    "# 词云生成\n",
    "def generate_wordcloud(word_list):\n",
    "    # 补全：生成词云图（需设置中文字体路径）\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='simhei.ttf',  # 字体路径（如无此文件需下载）\n",
    "        width=800, height=400,\n",
    "        background_color='white',\n",
    "        max_words=100\n",
    "    ).generate(' '.join(word_list))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.4** 基于主题模型的文本分析\n",
    "    阅读下面代码，观察运行的结果，解释出现这个结果的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# 下面的 url 是 csv 文件的远程链接，如果你缺失这个文件，则需要用浏览器打开这个链接\n",
    "url = 'https://raw.githubusercontents.com/Micro-sheep/Share/main/zhihu/answers.csv'\n",
    "# 本地 csv 文档路径\n",
    "csv_path = 'answers.csv'\n",
    "# 待分词的 csv 文件中的列\n",
    "document_column_name = '回答内容'\n",
    "pattern = u'[\\\\s\\\\d,.<>/?:;\\'\\\"[\\\\]{}()\\\\|~!\\t\"@#$%^&*\\\\-_=+a-zA-Z，。\\n《》、？：；“”‘’｛｝【】（）…￥！—┄－]+'\n",
    "# 下载 csv 文件\n",
    "if not os.path.exists(csv_path):\n",
    "    import requests\n",
    "    response = requests.get(url)\n",
    "    with open(csv_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "# 读取 csv 文件\n",
    "df = (\n",
    "    pd.read_csv(\n",
    "        csv_path,\n",
    "        encoding='utf-8-sig')\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={\n",
    "        document_column_name: 'text'\n",
    "    }))\n",
    "\n",
    "# 去重、去缺失、分词\n",
    "df['cut'] = (\n",
    "    df['text']\n",
    "    .apply(lambda x: str(x))\n",
    "    .apply(lambda x: re.sub(pattern, ' ', x))\n",
    "    .apply(lambda x: \" \".join(jieba.lcut(x)))\n",
    ")\n",
    "# 构造 TF-IDF\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(df['cut'])\n",
    "# 特征词列表\n",
    "feature_names = tf_idf_vectorizer.get_feature_names_out()\n",
    "# 特征词 TF-IDF 矩阵\n",
    "matrix = tf_idf.toarray()\n",
    "feature_names_df = pd.DataFrame(matrix,columns=feature_names)\n",
    "print(feature_names_df)\n",
    "# 特征词 TF-IDF 矩阵\n",
    "matrix = tf_idf.toarray()\n",
    "# 指定 lda 主题数\n",
    "n_topics = 5\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, max_iter=50,\n",
    "    learning_method='online',\n",
    "    learning_offset=50.,\n",
    "    random_state=0)\n",
    "# 核心，给 LDA 喂生成的 TF-IDF 矩阵\n",
    "lda.fit(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top_words_data_frame(model: LatentDirichletAllocation,\n",
    "                         tf_idf_vectorizer: TfidfVectorizer,\n",
    "                         n_top_words: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出每个主题的前 n_top_words 个词\n",
    "    '''\n",
    "    rows = []\n",
    "    feature_names = tf_idf_vectorizer.get_feature_names_out()\n",
    "    for topic in model.components_:\n",
    "        top_words = [feature_names[i]\n",
    "                     for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        rows.append(top_words)\n",
    "    columns = [f'topic {i+1}' for i in range(n_top_words)]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_to_data_frame(model: LatentDirichletAllocation, X: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出文档主题概率分布情况\n",
    "    '''\n",
    "    # 求出给定文档的主题概率分布矩阵\n",
    "    matrix = model.transform(X)\n",
    "    columns = [f'P(topic {i+1})' for i in range(len(model.components_))]\n",
    "    df = pd.DataFrame(matrix, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 n_top_words 个主题词\n",
    "n_top_words = 5\n",
    "top_words_df = top_words_data_frame(lda, tf_idf_vectorizer, n_top_words)\n",
    "display(top_words_df)\n",
    "\n",
    "# 转 tf_idf 为数组，以便后面使用它来对文本主题概率分布进行计算\n",
    "X = tf_idf.toarray()\n",
    "\n",
    "# 计算完毕主题概率分布情况\n",
    "predict_df = predict_to_data_frame(lda, X)\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分 基础练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 答案读取\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "with open('stdout.pkl', 'rb') as f:\n",
    "    stdout = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1** groupby()\n",
    "    给定DataFrame，求A列每个值的前3大的B的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': list('aaabbcaabcccbbc'), \n",
    "                   'B': [12,345,3,1,45,14,4,52,54,23,235,21,57,3,87]})\n",
    "\n",
    "##############################\n",
    "# TODO 使用一行代码实现，求出每个组的前3个最大值的和\n",
    "df1_1 = \n",
    "##############################\n",
    "\n",
    "df1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df1_1.equals(stdout['expect1_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    给定DataFrame，有列A, B，A的值在1-100（含），对A列每10步长，求对应的B的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame({'A': [1,2,11,11,33,34,35,40,79,99], \n",
    "                   'B': [1,2,11,11,33,34,35,40,79,99]})\n",
    "##############################\n",
    "# TODO 使用一行代码实现\n",
    "df1_2 = \n",
    "##############################\n",
    "\n",
    "df1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df1_2.equals(stdout['expect1_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2** agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Country':['China','China', 'India', 'India', 'America', 'Japan', 'China', 'India'], \n",
    "                'Income':[10000, 10000, 5000, 5002, 40000, 50000, 8000, 5000],\n",
    "                    'Age':[5000, 4321, 1234, 4010, 250, 250, 4500, 4321]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# TODO 使用一行代码实现，计算每个国家的平均收入和年龄\n",
    "df2_1 = \n",
    "##############################\n",
    "\n",
    "##############################\n",
    "# TODO 使用一行代码实现，计算每个国家的年龄和收入的最小值、最大值和平均值（注意年龄在前收入在后）\n",
    "df2_2 = \n",
    "##############################\n",
    "\n",
    "display(df2_1)\n",
    "display(df2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df2_1.equals(stdout['expect2_1'])\n",
    "assert df2_2.equals(stdout['expect2_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3** 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 示例数据\n",
    "data = {'Scores': [55, 70, 85, 90, 60, 75, 80, 95, 100, 65]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "##############################\n",
    "# TODO \n",
    "# 绘制箱线图，直接使用 df.plot() 方法\n",
    "\n",
    "##############################\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4** 中文分词与词频统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 jieba 分词，计算字符串 s 中的中文词汇个数，不包括中文标点符号。显示输分词后的结果，用”/ ”分隔，以及中文词汇个数。示例如下：\n",
    "\n",
    "输入：\n",
    "\n",
    "工业互联网实施的方式是通过通信、控制和计算技术的交叉应用，建造一个信息物理系统，促进物理系统和数字系统的融合。\n",
    "\n",
    "输出：\n",
    "\n",
    "工业/ 互联网/实施/ 的/ 方式/是/ 通过/ 通信/控制/ 和/ 计算技术/的/ 交叉/ 应用/建造/ 一个/ 信息/物理/ 系统/ 促进/物理/ 系统/ 和/数字/ 系统/ 的/融合/\n",
    "\n",
    "中文词语数是：27\n",
    "\n",
    "问题2：在问题1的基础上，统计分词后的词汇出现的次数，用字典结构保存。显示输出每个词汇出现的次数，以及出现次数最多的词汇。如果有多个词汇出现次数一样多，都要显示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "def process_text(s):\n",
    "    ##############################\n",
    "    # TODO \n",
    "    # 使用jieba对输入s分词\n",
    "    words = \n",
    "    # 使用正则表达式匹配中文标点符号（根据Unicode范围）\n",
    "    chinese_punct = \n",
    "    # 过滤标点符号\n",
    "    filtered = [w for w in words if not re.fullmatch(chinese_punct, w)]\n",
    "    ##############################\n",
    "    \n",
    "    # 拼接结果并统计数量\n",
    "    segmented = \"/ \".join(filtered) + \"/\"\n",
    "    return segmented, len(filtered)\n",
    "\n",
    "# 示例输入\n",
    "s = \"工业互联网实施的方式是通过通信、控制和计算技术的交叉应用，建造一个信息物理系统，促进物理系统和数字系统的融合。\"\n",
    "segmented_text, count = process_text(s)\n",
    "\n",
    "print(\"分词结果：\")\n",
    "print(segmented_text)\n",
    "print(\"中文词语数是：\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert segmented_text == stdout['expect1_4_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Counter(segmented_list):\n",
    "    word_counts = {}\n",
    "    ##############################\n",
    "    # TODO \n",
    "    # 统计词频\n",
    "    \n",
    "    ##############################\n",
    "    return word_counts\n",
    "\n",
    "def count_word_freq(segmented_list):\n",
    "    # 统计词频\n",
    "    word_counts = Counter(segmented_list)\n",
    "    # 找到最高频词\n",
    "    max_freq = max(word_counts.values(), default=0)\n",
    "    most_common = [k for k, v in word_counts.items() if v == max_freq]\n",
    "    return word_counts, most_common\n",
    "\n",
    "# 调用函数\n",
    "word_freq, top_words = count_word_freq(segmented_text.split(\"/ \")[:-1])  # 去除末尾空元素\n",
    "\n",
    "print(\"词频统计结果：\")\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq}次\")\n",
    "\n",
    "print(\"\\n出现次数最多的词汇：\")\n",
    "print(\"、\".join(top_words))\n",
    "\n",
    "display(word_freq)\n",
    "display(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert word_freq == stdout['expect1_4_2']\n",
    "assert top_words == stdout['expect1_4_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5** 基于主题模型的文本分析（LSI/LDA/NMF）\n",
    "\n",
    "非负矩阵分解（NMF）、潜在语义索引（LSI/SVD）和潜在狄利克雷分配（LDA）在主题建模中各有特点。\n",
    "\n",
    "\n",
    "### NMF：\n",
    "\n",
    "NMF: min∥V−WH∥ F 2 ​ , s.t. W , H ≥ 0 W,H≥0\n",
    "\n",
    "#### LSI:\n",
    "\n",
    "V≈UΣV T （最小化Frobenius范数）\n",
    "\n",
    "求解方法：数值优化（梯度下降、ALS等）或SVD分解。\n",
    "\n",
    "#### LDA：\n",
    "\n",
    "对每个文档 d d，从狄利克雷分布生成主题分布 θ d ∼ Dir ( α ) θ d ​ ∼Dir(α)；\n",
    "\n",
    "对文档中的每个词 w d i w di ​ ：\n",
    "\n",
    "采样主题 z d i ∼ Multinomial ( θ d ) z di ​ ∼Multinomial(θ d ​ )；\n",
    "\n",
    "采样词 w d i ∼ Multinomial ( ϕ z d i ) w di ​ ∼Multinomial(ϕ z di ​\n",
    "\n",
    "​ )。\n",
    "\n",
    "求解方法：变分推断（VI）、吉布斯采样（MCMC）或在线学习。\n",
    "\n",
    "#### 主题分析的结果本质对比：\n",
    "\n",
    "主题表示的本质区别\n",
    "\n",
    "NMF：\n",
    "\n",
    "主题是词的线性组合（如“体育=0.6×足球+0.3×比赛”），权重直接反映重要性。\n",
    "\n",
    "优点：直观，适合人工标注；缺点：无概率框架，难以量化其不确定性。（算法不保证收敛在最优解）\n",
    "\n",
    "LDA：\n",
    "\n",
    "主题是词的概率分布（如“体育: P(足球)=0.4, P(比赛)=0.3”）。\n",
    "\n",
    "优点：天然归一化，可计算生成概率；缺点：需调参（超参数 \n",
    "α\n",
    ",\n",
    "β\n",
    "α,β）。\n",
    "\n",
    "#### 实际应用选择：\n",
    "\n",
    "核心选择依据：\n",
    "\n",
    "可解释性：NMF ≈ LDA > LSI\n",
    "\n",
    "概率框架：仅LDA支持\n",
    "\n",
    "计算效率：NMF/LSI > LDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"自然语言处理是人工智能的核心技术，Transformer模型在机器翻译中表现出色。\",\n",
    "    \"社交媒体中的噪音数据需清洗标点，例如“绝绝子”等网络新词需要特殊处理。\",\n",
    "    \"在线教育平台通过分析学生作文，识别高频错误词汇如“的地得”混淆问题。\",\n",
    "    \"政府工作报告的长句结构需结合语义角色标注提取关键信息。\",\n",
    "    \"深度学习模型需要大量数据和算力支持，超导量子芯片可能解决这一瓶颈。\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 数据预处理与词袋模型构建\n",
    "   - 使用 jieba 分词并过滤停用词（停用词表需包含虚词、标点及领域相关词汇如“模型”“数据”）\n",
    "   - 构建 文档-词频矩阵（TF-IDF 或词袋模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "random.seed(42) \n",
    "\n",
    "custom_dict = \"\"\"人工智能 500 n\n",
    "超导量子芯片 300\n",
    "北京大学 1000 ns\n",
    "jieba 10 x\n",
    "动态新词 200 n\n",
    "\"\"\"\n",
    "with open(\"custom_dict.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_dict)  # 将自定义词典写入文件\n",
    "\n",
    "jieba.load_userdict(\"custom_dict.txt\")\n",
    "\n",
    "# 停用词表示例\n",
    "stopwords = [\"的\", \"是\", \"在\", \"需\", \"等\", \"例如\", \"结合\", \"问题\", \"，\", \"“\", \"”\"]\n",
    "\n",
    "# 分词与过滤\n",
    "def preprocess(texts):\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        ##############################\n",
    "        # TODO \n",
    "        # 分词,去掉停用词和长度小于2的词\n",
    "        words = \n",
    "        ##############################\n",
    "\n",
    "        processed.append(' '.join(words))\n",
    "    return processed\n",
    "\n",
    "processed_texts = preprocess(texts)\n",
    "display(processed_texts)\n",
    "##############################\n",
    "# TODO\n",
    "# 构建TF-IDF矩阵并输出\n",
    "vectorizer = \n",
    "dtm =\n",
    "##############################\n",
    "\n",
    "# 输出TF-IDF矩阵\n",
    "print(\"TF-IDF矩阵：\")\n",
    "display(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout['expect1_5_1_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert processed_texts == stdout['expect1_5_1_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SVD-LSI 实现\n",
    "    - 使用 TruncatedSVD 对dtm进行降维（主题数设为2），\n",
    "    - 输出每个主题的前3个关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "##############################\n",
    "# TODO\n",
    "# SVD降维（LSI） 两个主题，随机数种子42\n",
    "svd = \n",
    "lsa_topics = \n",
    "##############################\n",
    "\n",
    "# 输出主题词\n",
    "result5_2_1 = []\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for idx, component in enumerate(svd.components_):\n",
    "    top_terms = [terms[i] for i in component.argsort()[:-4:-1]]\n",
    "    print(f\"LSI主题{idx}: {', '.join(top_terms)}\")\n",
    "    result5_2_1.append(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result5_2_1 == stdout['expect1_5_2_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. NMF 主题合成\n",
    "   - 使用 NMF 分解矩阵（主题数设为2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# 这里就直接给出答案请大家学习\n",
    "\n",
    "nmf = NMF(n_components=2, init='nndsvd', random_state=42)\n",
    "nmf_topics = nmf.fit_transform(dtm)\n",
    "\n",
    "result5_3_1 = []\n",
    "for idx, topic in enumerate(nmf.components_):\n",
    "    top_terms = [terms[i] for i in topic.argsort()[:-4:-1]]\n",
    "    print(f\"NMF主题{idx}: {', '.join(top_terms)}\")\n",
    "    result5_3_1.append(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result5_3_1 == stdout['expect1_5_3_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LDA 主题建模\n",
    "    - 使用 gensim 训练 LDA 模型（主题数设为2）\n",
    "    - 输出文档-主题分布和主题-词分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 使用CountVectorizer替代gensim的词袋模型\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# 这里就直接给出答案请大家对比学习\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "\n",
    "# 输出主题词\n",
    "result5_4_1 = []\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[:-4:-1]  # 获取前3个词\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"LDA主题{topic_idx}: {', '.join(top_words)}\")\n",
    "    result5_4_1.append(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result5_4_1 == stdout['expect1_5_4_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分 进阶练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1** 电商平台销售数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "orders = {\n",
    "    '订单ID': [1001, 1002, 1003, 1004, 1005, 1006],\n",
    "    '客户ID': ['C001', 'C002', 'C003', 'C001', 'C004', 'C002'],\n",
    "    '产品类别': ['电子产品', '家居用品', '服装', '家居用品', '电子产品', '服装'],\n",
    "    '金额': [2999, 899, 459, 1299, 5999, 799],\n",
    "    '日期': ['2024-10-05', '2024-11-12', '2024-12-20', '2024-12-25', '2024-10-18', '2024-11-30']\n",
    "}\n",
    "\n",
    "customers = {\n",
    "    '客户ID': ['C001', 'C002', 'C003', 'C004', 'C005'],\n",
    "    '地区': ['北京', '上海', '广州', '深圳', '杭州'],\n",
    "    '注册日期': ['2020-05-01', '2021-08-15', '2022-03-22', '2023-11-10', '2024-09-01']\n",
    "}\n",
    "\n",
    "df_orders = pd.DataFrame(orders)\n",
    "df_customers = pd.DataFrame(customers)\n",
    "display(df_orders.head())\n",
    "display(df_customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 步骤1：数据合并\n",
    "##############################\n",
    "# TODO\n",
    "# 根据客户ID合并订单和客户数据\n",
    "merged = \n",
    "##############################\n",
    "display(merged.head())\n",
    "assert merged.equals(stdout['expect2_1_1'])\n",
    "\n",
    "# 步骤2：分组统计\n",
    "##############################\n",
    "# TODO\n",
    "# 按地区聚合，计算总销售额和平均订单金额，并重命名列\n",
    "region_stats = \n",
    "################################\n",
    "display(region_stats.head())\n",
    "assert region_stats.equals(stdout['expect2_1_2'])  # 保存地区统计数据\n",
    "\n",
    "# 多维度分组\n",
    "##############################\n",
    "# TODO\n",
    "# 按地区和产品类别聚合，计算订单量，并按地区和订单量排序，ascending=[True, False]\n",
    "product_stats = \n",
    "################################\n",
    "display(product_stats.head())\n",
    "assert product_stats.equals(stdout['expect2_1_3'])  # 验证产品统计数据\n",
    "\n",
    "# 步骤3：高级聚合\n",
    "##############################\n",
    "# TODO\n",
    "# 客户级分析，根据客户ID计算最大消费和最近消费日期\n",
    "customer_analysis = \n",
    "################################\n",
    "display(customer_analysis.head())\n",
    "assert customer_analysis.equals(stdout['expect2_1_4'])  # 验证客户分析数据\n",
    "\n",
    "##############################\n",
    "# TODO\n",
    "# 月度环比计算，按月份聚合，计算销售额和订单量，并计算环比增长率\n",
    "merged['日期'] = pd.to_datetime(merged['日期'])\n",
    "# 将日期设置为索引，并按月份重采样，计算销售额和订单量（订单量基于订单ID计算）\n",
    "monthly = \n",
    "# 计算订单量环比增长率，使用pct_change()方法，并乘以100转换为百分比\n",
    "monthly['增长率'] = \n",
    "################################\n",
    "display(monthly.head())\n",
    "assert monthly.equals(stdout['expect2_1_5'])  # 验证月度环比数据\n",
    "\n",
    "##############################\n",
    "# TODO\n",
    "# 步骤4：关联分析，计算注册时长与消费金额的相关性\n",
    "merged['注册日期'] = pd.to_datetime(merged['注册日期'])\n",
    "merged['注册时长'] = (pd.to_datetime('2024-12-31') - merged['注册日期']).dt.days\n",
    "# 计算相关性，使用corr()方法，并选择注册时长和金额列\n",
    "correlation = \n",
    "################################\n",
    "display(correlation)\n",
    "assert correlation == stdout['expect2_1_6']  # 验证相关性数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2** 文本主题分析与可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"领域\": [\"科技\"] * 10 + [\"医疗\"] * 10 + [\"法律\"] * 10,\n",
    "    \"内容\": [\n",
    "        \"量子计算芯片基于超导电路实现，其核心器件是超导约瑟夫森结，由超导体-绝缘介质-超导体构成，绝缘层厚度在纳米量级[[7]]。超导量子电路与现有集成电路技术高度兼容，支持量子比特的设计、制备和测量[[1]]。约瑟夫森结的非线性特性使超导谐振电路呈现非简谐能级分布，可单独控制能级跃迁以实现量子操作[[4]]。二维网络结构通过量子比特耦合单元共享公共比特实现扩展，提升芯片集成度[[3]]。\",\n",
    "        \n",
    "        \"超导量子芯片的量子比特通过瑟夫森结电路实现，其相干时间是衡量性能的核心指标之一。清华大学团队通过优化约瑟夫森结参数，将相干时间提升至毫秒量级[[5]]。超导量子计算机需结合量子纠错技术，目前主要采用表面码方案以对抗退相干[[10]]。\",\n",
    "        \n",
    "        \"超导量子芯片的耦合组件设计直接影响量子比特间的相互作用。通过调整耦合强度和频率，可实现可控的量子门操作[[2]]。SureCore公司使用低温晶体管模型设计存储器，支持在超导量子系统中构建模拟电路[[8]]。\",\n",
    "        \n",
    "        \"超导量子系统的量子纠错研究进展表明，表面码在超导平台上的实验实现已取得突破[[4]]。二维网络结构的超导电路通过公用比特扩展，为大规模量子计算提供了物理基础[[3]]。\",\n",
    "        \n",
    "        \"超导量子芯片的制造工艺需严格控制瑟夫森结的绝缘层厚度（纳米级）以确保超导特性[[7]]。量子比特的退相干主要由环境噪声和材料缺陷引起，需通过低温环境和屏蔽技术抑制[[10]]。\",\n",
    "        \n",
    "        \"超导量子计算的可扩展性依赖于量子比特间的低串扰设计。通过优化耦合组件的拓扑结构，可减少错误传播并提升系统稳定性[[2]]。目前主流方案采用超导电感和电容构建非线性振荡器作为量子比特[[1]]。\",\n",
    "        \n",
    "        \"超导量子芯片的量子门保真度是评估运算精度的关键指标。通过动态修正和实时反馈技术，当前实验已实现99.9%以上的单量子比特门保真度[[10]]。约瑟夫森结的非线性响应特性是实现量子比特能级跃迁的基础[[4]]。\",\n",
    "        \n",
    "        \"超导量子系统的量子态读取通常通过谐振腔耦合实现。读取保真度和速度直接影响量子算法的执行效率，需通过优化微波脉冲参数提升性能[[8]]。量子纠错编码需结合逻辑量子比特的冗余设计[[4]]。\",\n",
    "        \n",
    "        \"超导量子芯片的低温环境（接近绝对零度）是维持超导态的必要条件。稀释制冷机技术的进步使芯片工作温度稳定在10 mK以下，显著延长了量子态的相干时间[[10]]。超导量子比特的能级跃迁通过微波脉冲精确控制[[1]]。\",\n",
    "        \n",
    "        \"超导量子计算的软件工具链包括量子编程框架和低温电路设计环境。Semiwise的低温晶体管模型支持构建存储器和标准单元库，推动超导量子芯片的系统级集成[[8]]。\",\n",
    "        \n",
    "        \"肺结节的CT影像特征分析显示，恶性结节常呈现分叶、毛刺、胸膜牵拉等形态学特征[[2]]。结节直径>8mm且密度不均时，恶性概率显著升高（>60%），需结合PET-CT评估代谢活性[[5]]。治疗方案包括手术切除、立体定向放疗及靶向治疗，需根据患者PS评分和并发症风险综合选择[[6]]。\",\n",
    "        \n",
    "        \"肺结节CT影像的AI辅助诊断系统需处理对抗样本攻击。研究发现，通过添加高斯噪声或微小图案扰动可使模型误判率提升30%[[9]]。对抗训练和数据增强技术可提升鲁棒性，但需平衡诊断精度与计算开销[[6]]。\",\n",
    "        \n",
    "        \"肺结节的病理分型对治疗策略选择至关重要。腺癌占比最高（约40%），常表现为磨玻璃结节；鳞癌多呈实性结节伴空洞[[5]]。分子靶向治疗（如EGFR抑制剂）对驱动基因突变阳性患者有效率超70%[[3]]。\",\n",
    "        \n",
    "        \"肺结节的CT影像特征量化分析显示，最大径、体积倍增时间（VDT）和CT值是预测恶性的关键参数。VDT<100天提示恶性可能，需优先考虑穿刺活检[[2]]。三维重建技术可辅助评估结节与血管的解剖关系[[8]]。\",\n",
    "        \n",
    "        \"肺结节的CT影像伪影需与恶性特征区分。运动伪影可能导致边缘模糊，需通过薄层扫描（<1mm）和迭代重建技术减少[[6]]。微小钙化灶（<2mm）多为良性，而层状钙化提示错构瘤[[5]]。\",\n",
    "        \n",
    "        \"肺结节治疗的放射治疗方案中，立体定向放疗（SBRT）可实现90%局部控制率，但需严格勾画靶区以保护周围肺组织[[3]]。剂量验证需结合剂量计测量和蒙特卡洛模拟[[9]]。\",\n",
    "        \n",
    "        \"肺结节的CT影像AI诊断系统需符合《医疗AI三类证》要求。算法需在多中心数据集（>1000例）上验证敏感性和特异性，且对抗样本攻击下的性能下降需<10%[[6]]。\",\n",
    "        \n",
    "        \"肺结节的CT影像与病理对照研究显示，实性成分比例>50%的结节恶性概率达85%[[2]]。微小结节（<5mm）的随访间隔建议为6-12个月，需结合生长速率动态评估[[5]]。\",\n",
    "        \n",
    "        \"肺结节的CT影像特征与基因突变相关性研究表明，EGFR突变患者多表现为磨玻璃结节，KRAS突变则与实性结节关联显著[[3]]。液体活检（ctDNA）可辅助鉴别诊断但假阴性率较高（约30%）[[8]]。\",\n",
    "        \n",
    "        \"肺结节的CT影像AI系统需符合医疗设备安全标准（IEC 62304）。算法训练需使用标注数据（≥500例），且测试集需包含对抗样本（如GAN生成的伪影）以验证鲁棒性[[9]]。\",\n",
    "\n",
    "        \"《民法典》第1034条明确个人信息包括姓名、身份证号、生物识别信息等，要求数据处理者遵循合法、正当、必要原则[[1]]。某公司因未加密用户生物特征数据导致泄露，被判赔偿500万元并公开道歉[[4]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条的司法解释指出，匿名化处理后数据不适用该条款，但需通过GDPR标准评估匿名化程度[[3]]。典型案例中，某社交平台因将匿名数据与第三方ID关联，被认定违反该条款[[7]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条与《个人信息保护法》存在衔接关系。后者细化了告知同意规则，要求数据处理者以显著方式提示隐私政策[[1]]。某APP因默认勾选同意条款被认定违法，用户可主张撤销同意并要求删除数据[[4]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条在跨境数据传输中的适用引发争议。某企业将用户健康数据传输至境外服务器未履行安全评估，被网信办处罚款100万元[[7]]。法院判决指出，需符合第1038条规定的个人信息处理者义务[[3]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条的解释需结合比例原则。某大数据公司收集用户位置信息用于精准营销，法院认为超出必要范围，判令删除超范围收集的数据[[4]]。企业需建立数据分类分级制度以符合该条款[[9]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条对生物识别信息的保护引发技术挑战。某小区强制采集人脸数据被诉侵权，法院认定需提供替代验证方式以保障用户选择权[[7]]。技术方案需设计隐私计算框架（如联邦学习）以实现数据可用不可见[[8]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条与《网络安全法》共同构成数据安全法理基础。某电商平台因未及时修复漏洞导致数据泄露，被认定违反该条款并承担连带赔偿责任[[3]]。企业需建立数据安全事件应急响应机制[[6]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条的解释需考虑技术发展。某基因检测公司存储用户DNA数据未加密，法院援引该条款判令其承担侵权责任，并要求采用同态加密等技术保障安全[[4]]。技术方案需符合法律对数据处理的实质要求[[9]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条在跨境司法协助中的适用存在法律冲突。某跨国公司因未履行中国数据本地化要求，被认定违反该条款，其境外母公司承担连带责任[[7]]。需通过国际条约协调数据主权与个人隐私保护[[2]]。\",\n",
    "        \n",
    "        \"《民法典》第1034条对数据隐私的保护范围不包括公共信息。某明星住址因被曝光在房产登记中，法院认定不属于受保护的个人信息[[1]]。但若通过非法手段获取公共信息仍可能违反其他法律[[3]]。\"\n",
    "    ],\n",
    "    \"噪声标记\": [False] * 10 + [True] * 10 + [False] * 10\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 文本预处理：分词 + 停用词过滤  \n",
    "import jieba  \n",
    "stopwords = [\"的\", \"但\", \"了\", \"非常\", \"建议\"]  \n",
    "\n",
    "def preprocess(text):  \n",
    "    words = [word for word in jieba.lcut(text) if word not in stopwords and len(word) > 1]\n",
    "    return \" \".join(words)  \n",
    "\n",
    "df[\"分词结果\"] = df[\"内容\"].apply(preprocess)  \n",
    "\n",
    "# 2. 构建词袋模型  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "##############################\n",
    "# TODO\n",
    "# 初始化向量化器，最大特征数为100，使用TF-IDF对分词结果进行向量化\n",
    "tfidf = \n",
    "dtm =   \n",
    "##############################\n",
    "\n",
    "# 3. LDA主题提取  \n",
    "from sklearn.decomposition import LatentDirichletAllocation  \n",
    "##############################\n",
    "# TODO\n",
    "# 初始化LDA模型并拟合数据，主题数为2，随机数种子42\n",
    "lda = \n",
    "lda.fit(dtm)\n",
    "##############################\n",
    "\n",
    "# 4. 词频统计与可视化    \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "def Counter(segmented_list):\n",
    "    word_counts = {}\n",
    "    ##############################\n",
    "    # TODO \n",
    "    # 统计词频\n",
    "\n",
    "    ##############################\n",
    "    return word_counts\n",
    "\n",
    "##############################\n",
    "# TODO\n",
    "# 统计词频\n",
    "word_list = \" \".join(df[\"分词结果\"]).split()  \n",
    "word_counts = \n",
    "##############################\n",
    "result2_2_1 = word_counts\n",
    "\n",
    "# 绘制词云图  \n",
    "from wordcloud import WordCloud  \n",
    "##############################\n",
    "# TODO\n",
    "# 生成词云图，设置字体路径和背景颜色\n",
    "wordcloud = \n",
    "##############################\n",
    "plt.imshow(wordcloud)  \n",
    "plt.axis(\"off\")  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result2_2_1 == stdout['expect2_2_1']  # 验证词频统计数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
