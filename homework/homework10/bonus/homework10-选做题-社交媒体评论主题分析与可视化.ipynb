{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第十次课后练习 之二（选做）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**负责助教：朱轩宇**\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">请将作业文件命名为 第十次课后练习-选做题+姓名+学号.ipynb, 例如 第十次课后练习-选做题+张三+1000000000.ipynb</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 社交媒体评论主题分析与可视化\n",
    "\n",
    "## 引言\n",
    "在信息化社会背景下，社交媒体平台成为了人们表达情感、分享观点的重要渠道。通过对社交媒体评论进行情感分析，可以深入了解用户的态度和情感趋势，为市场策略、用户体验优化等提供有力支持。本文将详细介绍如何使用Python进行社交媒体评论的情感分析，并结合数据可视化技术，将分析结果直观地呈现出来。\n",
    "\n",
    "## 实验步骤\n",
    "1. 数据清洗\n",
    "   - 使用pandas读取数据文件，并进行数据清洗和预处理，包括去除重复值、正则清洗和分词。\n",
    "2. 主要关注点分析\n",
    "   - 计算词频并生成词云图，统计文本中词语的出现频率，并使用WordCloud库生成词云图展示结果。\n",
    "3. 主题分析\n",
    "   1. 进行一致性和困惑度计算，通过改变主题数量范围，计算不同主题数量下的一致性和困惑度，并绘制折线图展示结果。\n",
    "   2. 使用TF-IDF模型提取文本的关键词，计算每个关键词在文本中的权重，并输出前30个关键词。\n",
    "   3. 进行主题建模和关键词提取，使用LDA模型对分词结果进行主题建模，并提取每个主题的关键词。\n",
    "   4. 对主题建模结果进行可视化，使用pyLDAvis库生成LDA主题模型的可视化结果，并保存为HTML文件。\n",
    "   5. 聚类分析\n",
    "\n",
    "实验提供了两份数据，一个比较大，一个比较小，可以先使用小数据跑通之后换更大的数据运行。\n",
    "\n",
    "### 实验使用的关键库及版本\n",
    "python 3.10\n",
    "- jieba                     0.42.1\n",
    "- numpy                     1.26.4\n",
    "- pandas                    2.2.3\n",
    "- pyLDAvis                  3.4.1\n",
    "- scikit-learn              1.6.1\n",
    "- seaborn                   0.13.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as psg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pandas读取数据文件，并进行数据清洗和预处理，包括去除重复值、正则清洗和分词。\n",
    "print(\"Loading data...\")\n",
    "data=pd.read_csv(\"./data/doc_ewujushi_short.csv\")\n",
    "stopword_list = []\n",
    "with open(\"./stop_dic/stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        stopword_list.append(line.strip())  # 去除换行符和空格\n",
    "stopwords = set(stopword_list)\n",
    "\n",
    "# 定义分词函数\n",
    "def chinese_word_cut(mytext):\n",
    "    ##############################\n",
    "    # TODO \n",
    "    # 定义分词函数，实现去除非中文字符和停用词\n",
    "    mytext =    # 去除非中文字符\n",
    "    words =   # 使用jieba进行分词\n",
    "    ##############################\n",
    "    return \" \".join([word for word, flag in words if word not in stopwords])  # 去除停用词\n",
    "\n",
    "# 对数据进行处理\n",
    "print(\"Processing data...\")\n",
    "##############################\n",
    "# TODO \n",
    "# 对数据进行处理，包括去除重复值、缺失值以及重置索引\n",
    "  # 去除重复值\n",
    "  # 去除缺失值\n",
    "  # 重置索引\n",
    "##################################\n",
    "\n",
    "print(\"Cleaning data...\")\n",
    "data[\"content_cutted\"] = data.content.apply(chinese_word_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 探索性数据分析(词频分析)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for index, row in data.iterrows():\n",
    "    words = row[\"content_cutted\"].split()\n",
    "    for word in words:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "\n",
    "# 排序\n",
    "word_count_sorted = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "# 筛选高频词\n",
    "high_frequency_num = 10  # 设定筛选的高频词数量\n",
    "high_frequency_words = []\n",
    "##############################\n",
    "# TODO \n",
    "# 筛选高频词\n",
    "high_frequency_words =\n",
    "##############################\n",
    "high_frequency_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化展示\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 中文 字体设置\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False \n",
    "plt.rcParams.update({'font.size': 20}) # 设置字体大小\n",
    "\n",
    "##############################\n",
    "# TODO \n",
    "# 生成词频统计图，取前10个高频词\n",
    "high_frequency_words_to_draw =  # 取前10个高频词\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar() # TODO\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Statistics\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# TODO \n",
    "# 生成词云图\n",
    "wordcloud =\n",
    "##############################\n",
    "# 显示词云图\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LDA主题分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA主题模型分析\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##############################\n",
    "# TODO \n",
    "# 使用CountVectorizer进行文本向量化\n",
    "vectorizer = \n",
    "X = \n",
    "\n",
    "# 设定LDA模型参数，并进行拟合\n",
    "n_components = 5  # 主题数量\n",
    "lda = \n",
    "lda.fit(X)\n",
    "##############################\n",
    "\n",
    "# 输出主题词\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    tword = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        topic_w = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        tword.append(topic_w)\n",
    "        print(topic_w)\n",
    "    return tword\n",
    "\n",
    "n_top_words = 25\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "topic_word = print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "topics=lda.transform(X)\n",
    "topic = []\n",
    "##############################\n",
    "# TODO \n",
    "# 得到每篇文章对应主题 \n",
    "# 取主题概率最大的主题作为文章的主题\n",
    "\n",
    "##############################\n",
    "data['topic']=topic\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[0]#可以看出属于第几个话题的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "pyLDAvis.enable_notebook()                                  #在notebook中展示   \n",
    "pic = pyLDAvis.lda_model.prepare(lda, X, vectorizer)\n",
    "pyLDAvis.save_html(pic, 'lda_pass'+str(n_components)+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 困惑度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plexs = []\n",
    "scores = []\n",
    "n_max_topics = 10\n",
    "#################################\n",
    "# TODO\n",
    "# 计算困惑度和对数似然函数\n",
    "for i in range(1,n_max_topics):\n",
    "    print('正在进行第',i,'轮计算')\n",
    "    lda = \n",
    "    lda.fit(X)\n",
    "    plexs.append(lda.perplexity(X))\n",
    "    scores.append(lda.score(X))\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t=9                                      #区间最右侧的值。注意：不能大于n_max_topics\n",
    "x=list(range(1,n_t))\n",
    "plt.plot(x,plexs[1:n_t])\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t=9                                         #区间最右侧的值。注意：不能大于n_max_topics\n",
    "x=list(range(1,n_t))\n",
    "plt.plot(x,scores[1:n_t])\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 聚类分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类分析\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 使用TF-IDF方法表示文本特征\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(data[\"content_cutted\"])\n",
    "\n",
    "# 设定KMeans聚类参数\n",
    "n_clusters = 3  # 聚类数量\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 获取每个聚类的关键特征\n",
    "def get_top_keywords(model, feature_names, n_top_words):\n",
    "    \n",
    "    top_keywords = []\n",
    "    ##############################\n",
    "    # TODO \n",
    "    # 实现函数，获取每个聚类的关键特征\n",
    "    # 通过聚类中心的特征值来获取每个聚类的关键词\n",
    "    \n",
    "    ##############################\n",
    "    return top_keywords\n",
    "\n",
    "top_keywords = get_top_keywords(kmeans, vectorizer.get_feature_names_out(), 10)\n",
    "for i, keywords in enumerate(top_keywords):\n",
    "    print(f\"Cluster {i}: {', '.join(keywords)}\")\n",
    "\n",
    "# 获取每个样本所属的聚类\n",
    "data[\"cluster\"] = kmeans.labels_\n",
    "data[\"cluster\"].value_counts()  # 查看每个聚类的样本数量\n",
    "\n",
    "# 降维和可视化\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "##############################\n",
    "# TODO \n",
    "# 使用PCA降维到2维\n",
    "pca = \n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "##############################\n",
    "\n",
    "# 创建DataFrame用于可视化\n",
    "df = pd.DataFrame(X_pca, columns=[\"x\", \"y\"])\n",
    "df[\"cluster\"] = data[\"cluster\"]\n",
    "df[\"topic\"] = data[\"topic\"]\n",
    "# 可视化聚类结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"cluster\", palette=\"Set1\", alpha=0.7)\n",
    "plt.title(\"KMeans Clustering Results\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请合理使用AI工具对上述媒体内容进行综述，给出你的步骤（调用方案）以及提示词和结果。\n",
    "\n",
    "结果示例：\n",
    "\n",
    "国际局势动荡下的全球经济与市场反应\n",
    "——乌克兰危机升级，避险情绪推高黄金原油价格\n",
    "\n",
    "近期，乌克兰局势持续紧张，俄罗斯与西方国家的对峙进一步升级，引发全球市场剧烈震荡。据央视新闻报道，乌克兰东部地区冲突加剧，美国总统拜登与北约盟国宣布对俄罗斯实施新一轮经济制裁，导致国际油价和黄金价格大幅上涨。\n",
    "\n",
    "市场影响\n",
    "\n",
    "能源市场：受紧张局势影响，布伦特原油价格突破每桶100美元，欧洲天然气价格飙升。分析人士指出，俄罗斯作为全球主要能源出口国，若冲突持续，可能进一步扰乱供应链。\n",
    "\n",
    "避险资产：黄金价格创下近8个月新高，美元指数走强。金灿荣教授在接受采访时表示，市场避险情绪浓厚，投资者纷纷转向黄金等安全资产。\n",
    "\n",
    "股市震荡：美股三大指数集体下跌，纳指跌幅超2%。A股市场同样受到波及，沪指失守3500点。\n",
    "\n",
    "国际反应\n",
    "俄罗斯总统普京在公开讲话中指责北约东扩威胁国家安全，而乌克兰政府则呼吁国际社会提供更多支持。微博上，#乌克兰局势#话题阅读量突破10亿，网友热议战争风险与全球政治格局变化。\n",
    "\n",
    "社交媒体动态\n",
    "与此同时，国内社交媒体呈现两极分化：\n",
    "\n",
    "严肃讨论：部分用户转发央视新闻、专家解读，关注事件对经济的影响（如“黄金要不要现在买入？”）。\n",
    "\n",
    "娱乐化消解：另一部分网友以“二哈表情包”“哈哈哈”调侃紧张氛围，甚至将普京称为“雄狮”，形成“悲允”式黑色幽默。\n",
    "\n",
    "文化视角\n",
    "在文学社区，有读者推荐三叔的《落魄》全文，称其“值得一看”，并关联当前局势：“历史总是惊人相似，如同苏联解体时的沙姆事件。”\n",
    "\n",
    "编辑评述\n",
    "本次乌克兰危机不仅是地缘政治冲突，更是一场全球经济的压力测试。从市场反应看，能源与金融市场的波动揭示了全球化时代危机的传导性。值得注意的是，社交媒体上的分裂态度——严肃关注与娱乐化解构并存，反映了公众对复杂议题的多元应对策略。\n",
    "\n",
    "建议投资者：\n",
    "\n",
    "短期关注避险资产（黄金、美元），但警惕高位回调风险；\n",
    "\n",
    "长期需观察俄乌谈判进展及美联储政策变化。\n",
    "\n",
    "社会观察：当“普京”与“二哈”同屏出现时，我们或许该思考：娱乐化表达是缓解焦虑的方式，还是对严肃议题的消解？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
