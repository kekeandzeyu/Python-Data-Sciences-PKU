{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五次课后练习 之二（选做）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本次作业中，如果你在配置环境中遇到了问题，可以参考助教的解决方案：\n",
    "\n",
    "首先我们在将这个文件放在一个文件夹中（可以和这门课程的其他作业放在一起，但是别放在桌面这种一大堆其他文件的文件夹里）。\n",
    "\n",
    "随后运行：\n",
    "```\n",
    "python3 -m venv myenv\n",
    "source myenv/bin/activate\n",
    "pip install openai\n",
    "pip install requests\n",
    "pip install socksio\n",
    "```\n",
    "\n",
    "然后在 notebook 中切换 ipykernel 为 myenv（如果你采用的是 VSCode 的话，可以看看右上角的 ipykernel，点一下就可以切换了），就可以运行了。\n",
    "\n",
    "如果你发现还存在包缺失的情况，注意采用 Restart，不然环境可能同步不过来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**负责助教：吴迪**\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">请将作业文件命名为 第五次课后练习-之二+姓名+学号.ipynb, 例如 第五次课后练习-之二+张三+1000000000.ipynb</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  请认真阅读代码，理解学习代码的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.1** 读取网页内容，调用大语言模型API进行中文摘要\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "# 目标URL列表\n",
    "target_urls = [\n",
    "    \"https://arxiv.org/abs/2410.03761\",\n",
    "    \"https://arxiv.org/abs/2305.15186\"\n",
    "]\n",
    "\n",
    "# OpenRouter API配置，这个是提供免费deepseek服务的站点，\n",
    "# 也可以替换成直接访问deepseekAPI的版本\n",
    "API_ENDPOINT = \"https://openrouter.ai/api/v1\"\n",
    "API_KEY = \"<YOUR_API_KEY>\"\n",
    "\n",
    "# 正则表达式，用于解析网页摘要（读取html网页对象的content字段内容）\n",
    "pattern = r'<meta\\s+property=\"og:description\"\\s+content=\"(.*?)\"\\s*/>'\n",
    "\n",
    "# 初始化OpenAI客户端\n",
    "client = OpenAI(\n",
    "    base_url=API_ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "def generate_summary(text: str) -> str:\n",
    "    \"\"\"生成文本摘要\"\"\"\n",
    "    try:\n",
    "        # 调用DeepSeek API生成摘要\n",
    "        # 如果你采用了其他的站点的内容，你可能需要修改这里的 model 的内容。\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"deepseek/deepseek-chat:free\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Please write a 300 word(character) summary of the user's text in Chinese\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        # 返回生成的摘要内容\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        return \"Failed to generate summary\"\n",
    "\n",
    "def fetch_webpage_content(url: str) -> str:\n",
    "    \"\"\"获取网页内容\"\"\"\n",
    "    try:\n",
    "        # 发送HTTP GET请求获取网页内容\n",
    "        response = requests.get(url, timeout=10)  # 设置超时时间\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_summary(html_content: str) -> str:\n",
    "    \"\"\"从网页内容中解析出摘要\"\"\"\n",
    "    try:\n",
    "        # 使用正则表达式从HTML中提取摘要，读取html网页对象的content字段\n",
    "        match = re.search(pattern, html_content)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            raise Exception(\"No summary found in the webpage\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing summary: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # 遍历目标URL列表\n",
    "    \n",
    "    docs = \"\"\n",
    "    \n",
    "    for url in target_urls:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        \n",
    "        # 获取网页内容\n",
    "        html_content = fetch_webpage_content(url)\n",
    "        if not html_content:\n",
    "            print('access error')\n",
    "            continue  # 如果获取失败，跳过当前URL\n",
    "        \n",
    "        # 解析摘要\n",
    "        summary = parse_summary(html_content)\n",
    "        if not summary:\n",
    "            print('parse error')\n",
    "            continue  # 如果解析失败，跳过当前URL\n",
    "            \n",
    "        docs += summary\n",
    "        \n",
    "        # 生成中文摘要\n",
    "        chinese_summary = generate_summary(summary)\n",
    "        \n",
    "        # 打印结果\n",
    "        print(f\"Original Summary: {summary}\")\n",
    "        print(f\"Chinese Summary: {chinese_summary}\\n\")\n",
    "\n",
    "    chinese_summary = generate_summary(docs)\n",
    "    print(f\"Original message: {docs}\")\n",
    "    print(f\"multi doc Summary: {chinese_summary}\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 仿照上面的框架，实现一个自己版本的多文档（或多轮对话）的分析功能。可以用更复杂的爬虫获得需要的信息。也可以直接从本地读取数据文件实现有实际意义的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成有新意有创意工作的同学，或在作业过程中觉得有心得或者自己拓展学习到有价值内容的，可以在文件名最后加一个#号。例如第五次课后练习+张三+1000000000+#.ipynb\n",
    "只是完成学习的同学，没有尝试改进探索工作的可以不提交这个作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
